If you're building a computer vision application rather than training the ways from scratch, from random initialization, you often make much faster progress if you download ways that someone else has already trained on the network architecture and use that as pre-training and transfer that to a new task that you might be interested in. The computer vision research community has been pretty good at posting lots of data sets on the Internet so if you hear of things like Image Net, or MS COCO, or Pascal types of data sets and a lot of computer researchers have trained their algorithms on them. Sometimes this training takes several weeks and might take many GPUs and the fact that someone else has done this and gone through the painful process, means that you can often download open-source ways that took someone else many weeks or months to figure out and use that as a very good initialization for your own neural network. And use transfer learning to sort of transfer knowledge from some of these very large public data sets to your own problem.

Lets say you want to train your own ConvNet using one of the architectures you have studied to classify your 2 cats, Tigger and Misty. So, the task will have 3 classes, Tigger, Misty or neither. Now, you won't have many images of your cats, so your own training set will be small. It is recommended then, that you go online and download some open-source implementation of a neural network and download not just the code but also the weights. There are a lot of networks you can download that have been trained on for example, the Image Net data sets which has a thousand different classes so the network might have a softmax unit that outputs one of a thousand possible classes. What you can do is then get rid of the softmax layer and create your own softmax unit that outputs Tigger or Misty or neither.

In terms of the network, I'd encourage you to think of all of the layers as frozen, so you freeze the parameters in all of these layers of the network and you would then just train the parameters associated with your softmax layer. By using someone else's pretrained weights, you might probably get pretty good performance on this even with a small data set.

Fortunately, there are a lot of deep learning frameworks that supports this mode of operation and, in fact, depending on the framework you can set the number trainable parameters to zero for some of these early layers, in order to just say don't train these paramters. Some other frameworks will have something like freeze=1, etc.

One other neat trick that may help for some implementations is that because all of these early leads are frozen, there are some fixed-function that doesn't change because you're not training it, that takes the input image X and maps it to some set of activations in last layer before the softmax layer. So, one step that could help your computation as you just pre-compute the layers activation, for all the examples in the training set and save them to disk and then just train the softmax classifier right on top of that, then you don't need to recompute those activations everytime you take a epoch or take a pass through the training set. This is what you do if you have a pretty small training set for your task.

When you have a large training set, one rule of thumb is that you just freeze fewer layers. So, if your neural network has 5 layers, you freeze the first 3 and then train the last 2 layers along with the softmax layer that you will add. There are a few ways to do it. You can either unfreeze the weights of the last few layers and then update them through training, or you can just remove them and add your own FC layers along with the softmax layer. Either of these methods could be worth trying. But maybe one pattern is if you have more data, the number of layers you've freeze could be smaller and then the number of layers you train on top could be greater. And the idea is that if you pick a data set and maybe have enough data not just to train a single softmax unit but to train some other size neural network that comprises the last few layers of this final network that you end up using.

Finally, if you have a lot of data, one thing you might do is take this open-source network and ways and use the whole thing just as initialization and train the whole network. Again, you will have to remove the softmax layer of the original model and use your own softmax layer suited for your data, but you get the idea. 