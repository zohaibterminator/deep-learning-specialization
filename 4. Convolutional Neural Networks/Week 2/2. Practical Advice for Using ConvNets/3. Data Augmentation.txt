Most computer vision task could use more data. And so data augmentation is one of the techniques that is often used to improve the performance of computer vision systems. And this is true whether you're using transfer learning or using someone else's pre-trained ways to start, or whether you're trying to train something yourself from scratch.

Perhaps the simplest data augmentation method is mirroring on the vertical axis, where if you have an example in your training set, you flip it horizontally to get another image. And for most computer vision task, if the original picture is a cat then mirroring it will still be a cat. And if the mirroring operation preserves whatever you're trying to recognize in the picture, this would be a good data augmentation technique to use. Another commonly used technique is random cropping. So, you randomly take crops of the original image and make new data for your dataset. Random cropping isn't a perfect data augmentation. What if you randomly end up taking a crop which will not look much like a cat but in practice it is worthwhile so long as your random crops are reasonably large subsets of the actual image. You can also use other data augmentation techniques like rotation, shearing, local warping, etc.

The second type of data augmentation that is commonly used is color shifting. Lets say you have a RGB image, and you add some values to the R,G and B channels of the image. For example, we add 20 to the red and blue channels and subtract 20 from the green channel. So, red and blue make purple which makes the whole image a bit more purpley and that creates a distorted image for training set. In practice, you draw R, G and B from some probability distribution that could be quite small as well. But what you do is take different values of R, G, and B and use them to distort the color channels. There are different ways to sample R, G, and B. One of the ways to implement color distortion uses an algorithm called PCA. The details of this are actually given in the AlexNet paper, and it's sometimes called PCA Color Augmentation. But the rough idea for PCA Color Augmentation is for example, if your image is mainly purple, if it mainly has red and blue tints, and very little green, then PCA Color Augmentation, will add and subtract a lot to red and blue, whereas it will subtract very little to greens, so kind of keeps the overall color of the tint the same.

Now, this how normally data augmentation is implemented. So, you might have your training data stored in a hard disk. And if you have a small training set, you can do almost anything and you'll be okay. But if you have a very large training set, you might have a CPU thread that is constantly loading images off your hard disk. And what you can do is use maybe a CPU thread to implement the distortions, be it the random cropping, or the color shifting, or the mirroring, but for each image, you might then end up with some distorted version of it. So your CPU thread is constantly loading data as well as implementing whether the distortions are needed to form a batch, or really, mini batches of data. And this data is then constantly passed to some other thread or some other process for implementing training and this could be done on the CPU or really increasingly on the GPU if you have a large neural network to train. So, a pretty common way of implementing data augmentation is to really have one thread, or multiple threads, that is responsible for loading the data and implementing distortions, and then passing that to some other thread or some other process that then does the training, and often these processes can be done in parallel.

Similar to other parts of training a deep neural network, the data augmentation process also has a few hyperparameters such as how much color shifting do you implement and exactly what parameters you use for random cropping, etc.