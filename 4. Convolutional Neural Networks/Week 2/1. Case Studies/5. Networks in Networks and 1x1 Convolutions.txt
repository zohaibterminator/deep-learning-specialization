A convolution by a one-by-one filter doesn't seem particularly useful. You just multiply the image by some number, and that's it. But that's the case of 6x6x1 channel images. If you have a 6x6x32 instead of 6x6x1, then a convolution with a one-by-one filter can do something that makes much more sense. In particular, what a one-by-one convolution will do is it will look at each of the 36 different positions, and it will take the element-wise product between 32 numbers in the image and the 32 numbers in the filter, and then apply a ReLU nonlinearity to it after that.

In fact, one way to think about the 32 numbers you have in this 1x1x32 filter is as if you have one neuron that is taking as input 32 numbers from the image, and multiplying each of these 32 numbers, in one slice in the same position, height, and width, but 32 different channels, by 32 weights. More generally, if you have not just one filter, but if you have multiple filters, then it's as if you have not just one unit, but multiple units that is taking as input all the numbers in one slice and then building them up into an output of size 6 x 6 x # of filters. One way to think about a one-by-one convolution is that it is like basically having a fully connected neural network that applies to each of the 36 different positions. What that fully connected neural network does is it takes inputs 32 numbers and outputs number of filters, or N_c^[l+1]. This idea is often called a one-by-one convolution, but it's sometimes also called network in network.

To give an example of where one-by-one convolution is useful, here's something you could do with it. Let's say you have a 28x28x192 volume. If you want to shrink the height and width, you can use a pooling layer, so we know how to do that. But if the number of channels has gotten too big and you want to shrink that. How do you shrink it to a 28x28x32 dimensional volume? Well, what you can do is use, 32 filters that are one-by-one, and technically each filter would be of dimension 1x1x192, because the number of channels in your filter has to match the number of channels in your input volume. But you use 32 filters and the output of this process will be 28x28x32 volume. So, this is a way to shrink N_c. We'll see later how this idea of one-by-one convolutions allows you to shrink the number of channels and therefore save on computation in some networks. But of course, if you want to keep the number of channels to the 192, that's fine, too. The effect of a one-by-one convolution is it just has nonlinearity. It allows you to learn a more complex function of your network by adding another layer, the inputs 28x28x192, and outputs 28x28x192.