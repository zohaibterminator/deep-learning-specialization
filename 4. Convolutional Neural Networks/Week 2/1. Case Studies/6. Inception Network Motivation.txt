When designing a layer for a Conv layer, you might have to pick. Do you want a 1x3 filter or 3x3 or 5x5, or do you want a pooling layer? What the inception network does is that it says, why don't you do them all? This makes the network architecture more complicated, but it also works remarkably well.

Let's say for the second example that you have input a 28x28x192 dimensional volume. What the inception network or what an inception layer says is, instead of choosing what filter size you want in a conv layer or even do you want a convolutional layer or pooling layer, let's do them all. What if you can use a 1x1 convolution, and that will output a 28x28x64, for example. But maybe you also want to try a 3x3 Conv, and that might output a 28x28x128. Then what you do is just stack up this second volume next to the first volume. To make the dimensions match up, let's make the 3x3 Conv operation the 'same' convolution, so the output dimension is same as the input dimension in terms of height and width. Maybe you might say, well, I want to hedge my bets. Maybe a 5x5 filter works better so let's do that too, and have that output a 28x28x32. Again, you use the same convolution to keep the dimensions same. Maybe you don't want convolutional layer. Let's apply pooling, and that has some other output, and let's stack that up as well. Here, pooling outputs 28x28x32 (we will look at this later why this Max Pooling output is of 32 channels instead of 192). Now, in order to make all the dimensions match, you actually need to use padding for Max pooling. This is an unusual form of pooling because if you want the input to have height and worth 28x28 and have the output match the dimension and everything else also by 28x28, then you need to use same padding as well as a stride of one for pooling. This detail might seem a bit funny to you now, but let's keep going.

After applying everything and stacking up the volumes, we get our final output of 28x28x256. So, you will have one inception module input 28x28x192, and output 28x28x256. This is the heart of the inception network. The basic idea is that instead of you needing to pick the filter sizes or pooling you want and committing to that, you can do them all and just concatenate all the outputs and let the network learn whatever parameters it wants to use whatever combinations of these filter sizes it wants. Now, it turns out that there's a problem with the inception network, as we've described it here, which is computational cost. Let's figure out what's the computational cost of this 5x5 filter resulting in a 28x28x32 block.

You have 32 filters because the output has 32 channels, and each filter is going to be 5x5x192. The output size is 28x28x32, and so you need to compute 28x28x32 numbers, and for each of them, you need to do 5x5x192 multiplications. The total number of multipliers you need is the number of multipliers you need to compute each of the output values times the number of output values you need to compute. If you multiply out all these numbers, this is equal to 120 million. While you can do 120 million multipliers on a modern computer, this is still a pretty expensive operation. Now, we will see how using the idea of one-by-one convolutions, which you learned about previously, you'll be able to reduce the computational cost by about a factor of 10 to go from about 120 million multipliers to about one tenth of that.

Here's an alternative architecture for inputting 28x28x192 and outputting 28x28x32 which is following. You're going to input the volume, use a one-by-one convolution, to reduce the volume to a 16 channels instead of 192 channels, and then on this much smaller volume, run your 5x5 convolution to give you your final output. Notice, the input and output dimensions are still the same. You input 28x28x192 volume and get 28x28x32 volume as the output. But, what we have done is, instead of applying the 5x5 convolution operation directly on the 28x28x192 volume, we shrunk it to a 28x28x16 volume using 1x1 convolutions, and then applied 5x5 convolution on the smaller volume. The 28x28x16 volume is sometimes called bottleneck layer.

Now, let's look at the computational costs involved. To apply this one-by-one convolution, we have 16 filters. Each of the filters is going to be of dimension 1x1x192. This 192 filters matches the 192 channels of the input image. The cost of computing this 28x28x16 volume is going to be, well, you need 28x28x16 outputs. For each of them, you need to do 1x1x192 multiplications. If you multiply this out, this is 2.4 million.That's the cost of the first convolutional layer. The cost of the second convolutional layer will be that you have 28x28x32 outputs. Then for each of the outputs, you have to apply a 5x5x16 filter and if we multiply that out is equal to 10 million. The total number of multiplications you need to do is the sum of those, which is 12.4 million multiplications. So, you see we have effectively reduced the number of multiplications required from 120 million to 12.4 million multiplications using 1x1 convolutions.

Now, you might be wondering, does shrinking down the representation size so dramatically hurt the performance of your neural network? It turns out that as long as you implement this bottleneck layer so that within reason, you can shrink down the representation size significantly, and it doesn't seem to hurt the performance, but saves you a lot of computation.