The idea of MobileNet is everywhere that you previously have used an expensive convolutional operation. You can now instead use a much less expensive depthwise separable convolutional operation, comprising the depthwise convolution operation and the pointwise convolution operation.

The MobileNet v1 paper had a specific architecture in which it use a block of depthwise separable convolution operation 13 times. After these 13 layers, the neural network's last few layers were the Pooling layer, followed by a FC/Dense layer, followed by softmax layer to generate the final prediction. This turns out to perform well while being much less computationally expensive than earlier algorithms that used a normal convolutional operation.

Later, a better version of the MobileNet was released, named MobileNet v2. There were 2 main changes, one is the addition of residual connections, which is the same residual connections used in ResNets, and the second change is that it added this expansion layer which is added before the depthwise convolution operation, followed by pointwise convolution operation, which we will call projection in a pointwise convolution. This block is also called the bottleneck block and the MobileNet v2 repeated this block 17 times, followed by the same Pooling layer, FC/Dense layer, and the softmax layer.

In the MobileNet v2's bottleneck block, given an input N_h x N_w x 3, the MobileNet will pass it through the residual connection to the end of the block, as well as through the non-residual path of the block. Expansion operation will be applied to the input, convolution operation with 1x1xN_c filters, or in this case, 1x1x3 filters. But we will use large number of filters, so 18 in this case. So, we will end up with N_h x N_w x 18 volume. Expansion by 6 is quite typical in MobileNet V2, which is why our input went from a N_h x N_w x 3 to N_h x N_w x 18, expanded by a factor of 6. Then, we will apply the depthwise convolution operation, which will result in a N_h x N_w x 18 output, same dimension as our input. This is because we will use 'same' padding, which is why in a previous example, our output dimensions reduced after depthwise convolution operation. Then, we will apply the pointwise convolution operation using 1x1x18 sized filters. And if we use 3 filters, we will have a N_h x N_w x 3 volume. Which is why this pointwise convolution step is also projection layer, because you are projecting down from N_h x N_w x 18 to N_h x N_w x 3.

Bottleneck block accomplishes two things: One, by using the expansion operation, it increases the size of the representation within the bottleneck block. This allows the neural network to learn a richer function. But when deploying on a mobile device, on edge device, you will often be heavy memory constraints, the bottleneck block uses the pointwise convolution or the projection operation in order to project it back down to a smaller set of values, so that when you pass this the next block, the amount of memory needed to store these values is reduced back down. 