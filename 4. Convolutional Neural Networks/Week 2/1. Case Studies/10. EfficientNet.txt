MobileNet V1 and V2 gave you a way to implement a neural network, that is more computationally efficient. But is there a way to tune MobileNet, or some other architecture, to your specific device? Maybe you're implementing a computer vision algorithm for different brands of mobile phones with different amounts of compute resources, or for different edge devices. If you have a little bit more computation, maybe you have a slightly bigger neural network and hopefully you get a bit more accuracy, or if you are more computationally constraint, maybe you want a slightly smaller neural network that runs a bit faster, at the cost of a little bit of accuracy. How can you automatically scale up or down neural networks for a particular device? EfficientNet, gives you a way to do so.

Let's say you have a baseline neural network architecture, where the input image has a certain resolution r, and your new network has a certain depth d, and the layers has a certain width w. The authors of the EfficientNet paper observed that the three things you could do to scale things up or down, are, you could use a high resolution image, you could make the neural network deeper, or you could increase the width of the images. The question is, given a particular computational budget, what's the good choice of r, d, and w? Or depending on the computational resources you have, you can also use compound scaling, where you might simultaneously scale up or simultaneously scale down the resolution of the image, and the depth, and the width of the neural network. Now the tricky part is, if you want to scale up r, d, and w, what's the rate at which you should scale up each of these? Should you double the resolution and leave depth with the same, or maybe you should double the depth, but leave the others the same, or increase resolution by 10 percent, increase depth by 50 percent, and width by 20 percent? What's the best trade-off between r, d, and w, to scale up or down your neural network, to get the best possible performance within your computational budget?

If you are ever looking to adapt a neural network architecture for a particular device, look at one of the open source implementations of EfficientNet, which will help you to choose a good trade-off between r, d, and w.