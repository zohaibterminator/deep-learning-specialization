We have already seen all the basic building blocks of the Inception network. Now, let's see how you can put these building blocks together to build your own Inception network.

So the inception module takes as input the activation or the output from some previous layer. So let's say for the sake of argument the input is 28x28x192. The example we worked through in depth was the 1x1 followed by 5x5 layer. So, for example, the 1x1 has 16 channels and then the 5x5 will output a 28x28x32 channels. Then to save computation on your 3 by 3 convolution you can also do the same. And then the 3x3 conv outputs 28x28x128. And then maybe you want to consider a 1x1 convolution as well. There's no need to do a 1x1 conv followed by another 1x1 conv so there's just one step here and let's say these outputs 28x28x64. And then finally is the pooling layer. In order to really concatenate all of these outputs at the end we are going to use the same type of padding for pooling. So that the output height and width is still 28x28. So we can concatenate it with these other outputs. But notice that if you do max-pooling, even with same padding, 3 by 3 filter with stride of 1, the output will be 28x28x192. It will have the same number of channels and the same depth as the input that we had here. So what we're going to do is actually add one more 1x1 conv layer to shrink the number of channels. So, it gets us down to 28x28x32. And finally you take all of these blocks and you do channel concatenation. Just concatenate across this 64 + 128 + 32 + 32 and if you add it up this gives you a 28x28x256 dimensional output. So this is one inception module, and what the inception network does, is, more or less, put a lot of these modules together.

There's one last detail to the inception network if we read the original research paper. Which is that there are these additional side-branches. Well, the last few layers of the network is a fully connected layer followed by a softmax layer to try to make a prediction. What these side branches do is they takes some hidden layer and they try to use that to make a prediction. And you should think of this as maybe just another detail of the inception that's worked. But what is does is it helps to ensure that the features computed, even in the hidden units, even at intermediate layers, that they're not too bad for predicting the output cause of an image. And this appears to have a regularizing effect on the inception network and helps prevent this network from overfitting. And by the way, this particular Inception network was developed by authors at Google. Who called it GoogLeNet, spelled like that, to pay homage to the network.

FInally here's one fun fact. Where does the name inception network come from? The inception paper actually cites a meme for "we need to go deeper". And this URL http://knowyourmeme.com/memes/we-need-to-go-deeper [which no longer works :(] is an actual reference in the inception paper, which links to the image. And if you've seen the movie titled The Inception, maybe this meme will make sense to you. But the authors actually cite this meme as motivation for needing to build deeper new networks. And that's how they came up with the inception architecture. So I guess it's not often that research papers get to cite Internet memes in their citations. But in this case, I guess it worked out quite well.