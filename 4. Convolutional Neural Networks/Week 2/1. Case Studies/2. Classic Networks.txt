Lets start with LeNet-5. This neural network architecture was created to recognize hand-written digits. The original LeNet-5 NN was trained on grayscale images, and took 32x32x1 images as input. This architecture also inspired the NN architecture that was discussed at the end of week 1. The architecure contained 2 pairs of conv and pooling layers, and then 2 FC layer at the end with 120 neurons and 84 neurons each. A modern variant would have an extra layer at the end with 10 neurons with softmax classifier, as back then they used a different classifier, which is useless today. The original LeNet-5 architecture also used average pooling instead of max pooling, because back then people used average pooling much more. So, if you are building a modern variant, you should probably use max pooling instead. Also, in the original architecture, no padding was used as back then people were using valid padding or no padding, so as you go deeper into the network the feature map's size shrinks. This neural network is considered small by modern standards, as it had about 60K parameters compared to a usual neural network that is made today which has 10 to 100 million parameters. Some notes if you read the original paper are that back then people used Sigmoid and tanh activation functions instead of ReLU. Another note, is that the original LeNet way has a crazy complicated way of actually applying the convolution operation. Usually, if you have N_h x N_w x N_c image, you would have a f x f x N_c filter with the same number of channels as the image. But, since computers were slower, LeNet came up with a way to skip these large computations is that they would have different filters that looked at different channels, just so they could avoid the large matrix computations. Obviously, modern implementation would not have that kind of complexity. The last thing which was used in LeNet-5 which is not used now-a-days is they used activation functions after applying pooling. Specifically, they used sigmoid activation in LeNet-5 after pooling layers.

Next ConvNet we will look at is the AlexNet, named after Alex Krizhevsky who authored the paper along with few other scientists. The images that the AlexNet was trained on were of 227x227x3 size, but if you read the paper it says the sizes of the images were 224x224x3, but if you look at the numbers, the numbers only actually makes sense if the size of the images are 227x227x3. It had 5 conv layers and 3 Max pooling layers. The final feature map was of size 6x6x256 which is flattened to form a feature vector of 9216 values, which is then passed through 2 FC layers having 4096 neurons each, and finally, a final layer with 1000 neurons and a softmax activation function for classification. This NN actually had a lot of similarities to LeNet-5, but it was much bigger. LeNet had 60k parameters, AlexNet had about 60million parameters. And the fact that it could take pretty similar building blocks that have a lot more hidden units and trained on a lot more data allowed it to have remarkable performance. One other feature that made AlexNet better than LeNet is the use of ReLU activation function. Now, if you read the paper, you will see that this neural network was trained on two GPUs and the layers were actually split across GPUs, so there was a thoughtful way for when the two GPUs would communicate with each other. This is because back then GPUs were not fast enough to train a NN of this size efficiently. This is less of a concerned now as you can train AlexNet or a modern counterpart easily on a GPU. Apart from this, the paper also states that the ALexNet architecture also had another set of a layer called Local Response Normalization. This type of layer isn't really used much. What Local Response Normalization (LRN) does is, for example, if you had a 13x13x256 block, you look at one position. So one position at height and width, and look down this across all the channels, so all 256 numbers and normalize them. And the motivation for this Local Response Normalization was that for each position in this 13 by 13 image, maybe you don't want too many neurons with a very high activation. But subsequently, many researchers have found that this doesn't help that much.

The last ConvNet we will discuss is the VGG-16 network. The remarkable thing about VGG-16 is that instead of having a lot of hyperparameters, they focused on building a simple Neural network in which they focused on having Conv layers with 3x3 filters, stride 1, and same padding, and Max pooling layers with 2x2 filters and stride 2. So, the 16 layers in the VGG-16 actually represents that there are 16 layers in the VGG-16 that have weights, so there are 13 conv layers + 2 FC layers + 1 output layer. The classification head is the same as in AlexNet, 2 FC layers with 4096 neurons each, and a softmax layer with 1000 neurons. Overall, VGG-16 has 21 layers, including 5 max-pooling layers. So, it is a pretty large network and has a total of 138 million parameters, which is big even by modern standards. But the simplicity of the VGG-16 architecture made it quite appealing. You can tell this architecture is really quite uniform. There is a few conv-layers followed by a pooling layer, which reduces the height and width. But then also, if you look at the number of filters in the conv-layers, here you have 64 filters and then you double to 128 double to 256 doubles to 512. This sort of roughly doubling on every step, or doubling through every stack of conv-layers was another simple principle used to design the architecture of this network. he main downside was that it was a pretty large network in terms of the number of parameters you had to train. And if you read the literature, you sometimes see people talk about the VGG-19, that is an even bigger version of this network. But because VGG-16 does almost as well as VGG-19, a lot of people will use VGG-16. This made this pattern of how, as you go deeper and height and width goes down, it just goes down by a factor of two each time for the pooling layers whereas the number of channels increases. And here roughly goes up by a factor of two every time you have a new set of conv-layers.