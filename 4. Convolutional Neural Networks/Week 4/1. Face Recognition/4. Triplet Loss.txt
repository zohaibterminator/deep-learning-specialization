One way to learn the parameters of the neural network, so that it gives you a good encoding for your pictures of faces, is to define and apply gradient descent on the triplet loss function.

To apply the triplet loss you need to compare pairs of images. For example, given two pictures of the same person, you want their encodings to be similar because they are images of the same person. Whereas given pair of images of different persons, you want their encodings to be quite different. In the terminology of the triplet loss, what you're going to do is always look at one anchor image, and then you want the distance between the anchor and a positive image, really a positive example, meaning the image of the same person, to be similar. Whereas you want the distance of the anchor image and negative image to be much further apart. This is what gives rise to the term triplet loss, which is that you always be looking at three images at a time. You'll be looking at an anchor image, a positive image, as well as a negative image.

To formalize this, what you want is for the parameters of your neural network or for your encoding to have the following property; You want the encoding between the anchor minus the encoding of the positive example (d(A, P)) to be small, and in particular, you want this to be less than or equal to the distance or the squared norm between the encoding of the anchor and the encoding of the negative image (d(A, N)):

d(A, P) <= d(A, N)

or

||f(A) - f(P)||^2 <= ||f(A) - f(N)||^2

Now, if we move the term from the right side of this equation to the left side, what you end up with is:

||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 <= 0

But now we're going to make a slight change to this expression, which is; one trivial way to make sure this is satisfied is to just learn everything equals zero. If the feature vectors of all images are all zeros, then the equation is satisfied. So, by saying f of any image equals a vector of all zero's, you can see almost trivially satisfy this equation. Another way for the neural network to give a trivial outputs is if the encoding for every image was identical to the encoding to every other image. This would mean the distance between all the images is zero, satisfying the equation. To make sure that the neural network doesn't just output zero, for all the encodings, or to make sure that it doesn't set all the encodings equal to each other, we're going to do is modify this objective to say that the difference between the two norms doesn't need to be just less than equal to zero, it needs to be quite a bit smaller than zero. In particular, if we say the expression needs to be less than negative α, where α is another hyperparameter, hen this prevents a neural network from outputting the trivial solutions. Conventionally, we write the equation as:

||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + α <= 0

This is also called a margin, which is terminology that you'd be familiar with if you've also seen the literature on support vector machines, but don't worry about it if you haven't.

Given example, where the margin is set to 0.2 and d(A, P) is 0.5, then you won't be satisfied if d(A, N) was just a little bit bigger, say 0.51. Even though 0.51 is bigger than 0.5, you're saying that's not good enough. We want d(A, N) to be much bigger than d(A, P). In particular, you want it to be at least 0.7 or higher. That's what having a margin parameter here does. Which is it pushes the anchor-positive pair and the anchor-negative pair further away from each other.

Let's formalize the triplet loss function. The triplet loss function is defined on triples of images. Given three images: A, P, and N, the anchor positive and negative examples, so the positive examples is of the same person as the anchor, but the negative is of a different person than the anchor. The loss is that we will take the max of the equation that we discussed above with the margin hyperparameter, and zero, because if the equation is less than equal to zero, than the NN has achieved it's goal and the loss is 0. But if on the other hand, if the expression is > 0, then if you take the max, the max will end up selecting the value of the expression. So, the loss function is:

L(A, P, N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + α, 0)

This is the loss of one triplet. The overall cost function can be defined over the sum of the losses of all the m triplets:

J(A, P, N) = ∑L(A^(i), P^(i), N^(i))

If you have a training set of say, 10,000 pictures with 1,000 different persons, what you'd have to do is take your 10,000 pictures and use it to generate triplets, and then train your learning algorithm using gradient descent on this type of cost function, which is really defined on triplets of images drawn from your training set. Notice that in order to define this dataset of triplets, you do need some pairs of A and P, pairs of pictures of the same person. For the purpose of training your system, you do need a dataset where you have multiple pictures of the same person. That's why in this example I said if you have 10,000 pictures of 1,000 different persons, so maybe you have ten pictures, on average of each of your 1,000 persons to make up your entire dataset. If you had just one picture of each person, then you can't actually train this system. But of course, after having trained a system, you can then apply it to your one-shot learning problem where for your face recognition system, maybe you have only a single picture of someone you might be trying to recognize.

Now, how do we actually choose the triplets? One of the problems is if you choose A, P, and N randomly from your training set, subject to A and P being the same person and A and N being different persons, one of the problems is that if you choose them so that they're random, then this constraint is very easy to satisfy. Because given two randomly chosen pictures of people, chances are A and N are much different than A and P, and there's a very high chance that d(A, N) will be much bigger, more than the margin helper, than d(A, P) + α, and the Neural Network won't learn much from it. To construct your training set, what you want to do is to choose triplets, A, P, and N, that are ''hard'' to train on. A triplet that is ''hard'' would be if you choose values for A, P, and N so that may be d(A, P) is actually quite close to d(A, N). In that case, the learning algorithm has to try extra hard to take d(A, N) and try to push it up or d(A, P) and try to push it down so that there is at least a margin of α between the left side and the right side. The effect of choosing these triplets is that it increases the computational efficiency of your learning algorithm. If you choose the triplets randomly, then too many triplets would be really easy and gradient descent won't do anything because you're Neural Network would get them right pretty much all the time. It's only by choosing ''hard'' to triplets that the gradient descent procedure has to do some work to try to push the two quantities apart.

Now, it turns out that today's Face recognition systems, especially the large-scale commercial face recognition systems are trained on very large datasets. Datasets north of a million images are not uncommon. Some companies are using north of 10 million images and some companies have north of a100 million images with which they try to train these systems. These are very large datasets, even by modern standards, these dataset assets are not easy to acquire. Fortunately, some of these companies have trained these large networks and posted parameters online. Rather than trying to train one of these networks from scratch, this is one domain where because of the sheer data volumes sizes, it might be useful for you to download someone else's pre-trained model rather than do everything from scratch yourself. But even if you do download someone else's pre-trained model, I think it's still useful to know how these algorithms were trained in case you need to apply these ideas from scratch yourself for some application.