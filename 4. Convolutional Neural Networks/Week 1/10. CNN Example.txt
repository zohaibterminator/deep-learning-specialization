(Look at the CNN example image beforehand)

Lets say you have to make a neural network for hand-written digits recognition. You then create a NN with 2 Conv-Pool layers, and 2 FC/Dense layers followed by a softmax layer for the final prediction. You can see in the image that the Conv layer and Max pooling layer is considered 1 layer. So, it turns out that in the literature of a ConvNet there are two conventions which are inside the inconsistent about what you call a layer. One convention is that this is called one layer. So this will be layer one of the neural network, and another convertion will be to call the conv layer as a seperate layer and the pool layer as a seperate layer. When people report the number of layers in a neural network usually people just record the number of layers that have weight, that have parameters. And because the pooling layer has no weights, has no parameters, only a few hyper parameters, I am going to use a convention that Conv 1 and Pool 1 shared together and I am going to treat that as Layer 1.

One common guideline is to actually not try to invent your own settings of hyper parameters, but to look in the literature to see what hyper parameters you work for others. And to just choose an architecture that has worked well for someone else, and there's a chance that will work for your application as well. As you go deeper in the neural network, usually the height and width of the output matrix of each layer will decrease, whereas the number of channels will increase. And another pretty common pattern you see in neural networks is to have conv layers, maybe one or more conv layers followed by a pooling layer, and then one or more conv layers followed by pooling layer, and then at the end you have a few fully connected layers and then followed by maybe a softmax or a logistic regression unit.

Now, if you look at the table for the activation shapes, size and number of parameters, you will see that the pooling layers do not have any parameters. They have hyperparameters, as discussed earlier but no learnable parameters. Secondly, conv layers tend to have fewer parameters compared to the FC/Dense layers. Then, you will also see that the activation size gradually decreases as you go deeper into the NN. If it drops too quickly, than it is not great for performance. You will find that a lot of ConvNets will have properties and patterns similar to these.