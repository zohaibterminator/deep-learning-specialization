Now lets take a look at a simple conv neural network. Let's say you have an image, and you want to do image classification, or image recognition. Where you want to take as input an image, x, and decide is this a cat or not, 0 or 1, so it's a classification problem. Let's say this image is 39 x 39 x 3. We decide that we want to make a neural network with 3 conv layers. In layer 1, we decided to have 10 3x3 filters, stride 1 and valid padding (no padding). The resultant matrix would be of size 37x37x10, whose height and width we calculated using the formula and the number of channels match the number of filters we used. Then, in layer 2, we had 20 5x5 filters, stride 2 and valid padding. The resultant matrix would be a 17x17x20. In the last conv layer, we had 40 5x5 filters, with stride 2 and, again, valid padding. The resultant matrix would then be 7x7x40.

Now, to classify the image as a cat or not, we would then flatten the image to a vector of size 1960 values (7x7x40 = 1960), then pass it through a FFNN, with a single neuron in the output layer that uses a sigmoid activation function which classifies the image as a cat or not.

It turns out, there are 3 types of layers in a convolutional neural network, the conv layer, a pooling layer, and a fully connected layer or dense layer. We will look at the other 2 layers now.