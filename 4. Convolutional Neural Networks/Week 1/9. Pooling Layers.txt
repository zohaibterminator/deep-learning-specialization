Other than convolutional layers, ConvNets often also use pooling layers to reduce the size of the representation, to speed the computation, as well as make some of the features that detects a bit more robust.

Suppose you have a 4x4 input, and you want to apply a type of pooling caled Max pooling. The output of this particular implementation of Max pooling will be a 2x2 output. What you will do is think of the convolution operation as in you have a filter of 2x2 size, in this case, and you place it on the top left portion of the input, and instead of the applying the regular convolution operation, you just take the maximum value in that 2x2 region, and that will be your first value of your 2x2 resultant matrix. Then, take a stride of 2, and repeat the whole process. In this way, you will get a 2x2 resultant matrix which have the maximum values from each of the 2x2 regions of the input. The same formula we learned to calculate the output size of the matrix of the convolution output, applies to the max pooling operation as well. For 3D images, the max pooling operation is applied on all the channels seperately. So, the number of channels in the output matrix are the same as in the input.

Here's the intuition behind what max pooling is doing. If you think of this four by four region as some set of features, the activations in some layer of the neural network, then a large number  means that it's maybe detected a particular feature. So, the upper left-hand quadrant has this particular feature. It maybe a vertical edge or maybe a eye or whisker if you trying to detect a cat. Clearly, that feature exists in the upper left-hand quadrant. What the max operation does is if there are features that are detected in each of the quadrants, it preserves it in the output of the max pooling.

One good property of the pooling is that even though it has some hyperparameters (size of filter and strides) it has no parameters to learn. Once you fix the size of filter and the stride, it is a fixed computation and gradient descent doesn't change anything.

Average pooling is another type of pooling that is not used very often. In this type of pooling, instead of taking the max value we take the average of all the values in the fxf region. These days, max pooling is used much more often than average pooling with one exception, which is sometimes very deep in a neural network. You might use average pooling to collapse your representation from say, 7 by 7 by 1,000. An average over all the spacial sense , you get 1 by 1 by 1,000. We'll see an example of this later.Min pooling is also another type of pooling, in which we take the min values instead of the max or average.