We learned how do we convolve a 3D volume with multiple filters. Now, if we take the previous example, the final thing we will add to complete this one layer of a Convolutional Neural Network, is we will add bias. We will add different biases, through python broadcasting, to each of the 4x4 matrices that we got after convolution. Then, we will pass each of those 4x4 matrices through an activation function, for example ReLU, after adding bias and then finally we will stack the matrices to get our final 4x4x2 matrix.

Lets say we have 1 filters of 3x3x3 size in a layer of a neural network. How many parameters will the layer have? The answer is 280. Because, we know that each 3x3x3 filter will have 28 parameters (27 weights and 1 bias). We also know there are 10 filters. So, the total number of parameters in the layer are 28x10, which is 280. This is what makes Convolutional neural networks less prone to overfitting compared to a feed forward neural network when it comes to working with images, because we can work with large images with a relatively small number of parameters.