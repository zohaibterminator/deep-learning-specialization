In order to build up to object detection, you will need to first learn about object localization. We already know what is image classification, for e.g. where an algorithm looks at an image and decides whether the picture is of a car, or of some other object. Object detection problem is basically 2 problems grouped into one, object classification and object localization. So, not only an algorithm looks at an image and decides whether the image is of a car, it also has to identify where exactly is the object in the image using something we call bounding box, which is basically a rectangle around the position of the object. The object localization problem refers to figuring out where in the image the object is located. There can be multiple instances of the object in an image in object detection, and the NN will need to locate them all. The terminology we will use is that the classification and the classification with localization problems usually have one object that you're trying to recognize or recognize and localize. In contrast, in the detection problem there can be multiple objects. And in fact, maybe even multiple objects of different categories within a single image. So the ideas you've learned about for image classification will be useful for classification with localization. And that the ideas you learn for localization will then turn out to be useful for detection.

We already know the basic NN structure for an image classification task. So if you are building a self driving car, maybe your object categories are the following. Where you might have a pedestrian, or a car, or a motorcycle, or a background, which means none of the above. We will have a ConvNet, into which you will input a picture. And this results in a vector features that is fed to maybe a softmax unit that outputs the predicted class, which is one of the 4 classes. If you want to localize the object in the image as well, you can change your NN to have a few more output units that will output a bounding box, which consists of 4 numbers which are bx, by, bw, and bh. I am going to use the notational convention that the upper left of the image will be denoted as the coordinate (0,0), and at the lower right is (1,1). So, for specifying the bounding box, the red rectangle requires specifying the midpoint. So that’s the point (bx, by) as well as the height, that would be bh, as well as the width, bw of this bounding box.

Let's formalize this a bit more in terms of how we define the target label y for this as a supervised learning task. So, we have our 4 classes, and the NN will output an additional 4 numbers for the bounding box. Our y label will be basically a vector, which will the value Pc, which is the probability whether an object exists in the image, the 4 values for the bounding box which were bx, by, bw, and bh, and finally, if Pc is not zero (meaning an object exists), the NN will output 3 additional values, which will indicate whether the object belongs to the pedestrian class, the car class, or the motorcycle class. Remember in the problem we're addressing we assume that your image has only one object. So at most, one of these objects appears in the picture, in this classification with localization problem. The loss function will be defined as follows if we are just using squared error:

if y_1 = 1: (ŷ_1-y_1)^2 + (ŷ_2-y_2)^2 + (ŷ_3-y_3)^2 + ... (ŷ_8-y_8)^2
if y_1 = 0: (ŷ_1-y_1)^2

So, if the truth label for Pc is 1, meaning if an object exists, then we will sum the squared error for each output, and if the truth label for Pc is 0, we will only take the squared error for the Pc, because we don't care what values for bounding box and class labels the NN has predicted since there is no object in the image. Just as a side comment, I've used the squared error just to simplify the description here. In practice you could probably use a log-like feature loss for the c1, c2, c3 to the softmax output. Usually you can use squared error or something like squared error for the bounding box coordinates and for Pc you could use something like the logistics regression loss. Although even if you use squared error it'll probably work okay.