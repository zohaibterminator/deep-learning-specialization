When you are training a model using normal or batch GD, an model that is training normally, will have it's cost being constantly reduced. But if you train a model using mini-batch GD, you will see that the cost function plot will also be constantly decreasing, but be more noisier. This is because in each iteration, its like the model is being trained on a different dataset or mini-batch each time.

One parameter you will have to choose is the mini-batch size. One extreme is that if the mini-batch size is m, than thats just the batch GD. Another extreme is setting the mini-batch size to 1, which will give you another algorithm called Stochastic gradient descent, where every example is it's own mini-batch. If we see how these two algorithm optimize, then with batch GD, it will take a pretty low-noise path the global minimum and will take large steps at first, than will slow down when it gets closer to the global minimum. But it will take a lot of time to do that if you working with a large dataset. With Stochastic GD, it will be a lot noisier. Because, if you are looking at each example seperately, than sometimes an example will put the algorithm in the direction of the global minimum, sometimes it will not. A Stochastic GD algorithm will also never converge to a global minimum, because it will keep changing it's direction with each iteration. Although that effect can be slightly reduced by setting a smaller learning rate, you will lose the speed and efficiency vectorization brings.

We have to pick the mini-batch size that falls somewhere in between both of them. Because by doing that you get the best of both worlds. You get the vectorization, as you can now process multiple examples quickly using vectorization, but you can also make progress faster and dont have to wait to process the entire dataset to make some progress. Although, mini-batch GD is also noisey and doesnt head to the minimum all the time, and also always converge or oscillate in a very small region, which can be mediated by using learning rate decay (more on that later).

Here are some guidelines for choosing the mini-batch size:

* If you are using a smaller training set, maybe a training set with less than 2000 examples, just use batch GD. The training speed is not worth it here as you can still process data quickly using batch GD.

* If you have a large training set, typical mini-batch sizes are 64, 128, 512, etc. Because of how computer memory is layed out accessed, your mini-batch GD rins faster if the mini-batch size is in the power of 2.