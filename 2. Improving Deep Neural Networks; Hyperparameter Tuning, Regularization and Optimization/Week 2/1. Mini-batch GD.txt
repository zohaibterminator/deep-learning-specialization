Since applied ML is a emphirical process, in which you will be training a lot of models to find the best one. This is why having a good optimization algorithm is important, because it will train your model faster, which will allow you to train and test models more quickly.

One such optimization algorithm is mini-batch GD. In the normal GD, you will process all m examples before making a small step towards the global optima. Even with vectorization, if you have 5 million examples, you would have to process the entire 5 million examples for each step of GD. But with mini-batch GD, you will split your training set into small training sets called mini-batches. For example, each of the training set has 1000 examples. Therefore, if you split up a training set with 5 million examples into mini-batches with 1000 examples, there will be total 5000 mini-batches. You will split up the features X as well as the corresponding Y labels. Each mini-batch is represented by X^{i} for the features, and Y^{i} for the labels.

For mini-batch GD, the process is tha same as the normal GD, but instead you will be processing 1000 examples at a time, instead of all 5 million examples at once. But you will instead use a for loop from 1 to 5000 to iterate over all 5000 mini-batches.