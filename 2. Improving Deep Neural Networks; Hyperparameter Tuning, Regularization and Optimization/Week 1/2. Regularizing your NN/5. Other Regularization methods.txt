As we know that one sure way of reducing overfitting is to get more data, but we also know that it isnt always possible. Lets say you are making a cat classifier, and your algo is overfitting. But what you can do is augment your dataset and take an image, and, for example, flip it and add it to your dataset. So, if you do this for every image, you can basically double your dataset. Because the training set can become redundant, this isnt as good as acquiring new data, but you could do this without needing to pay the expense of going out to take more pictures of cats.

Apart from just flipping, you can apply many operations on the data. You can also zoom in and then crop the image, and through taking random distortions of the image and adding it to your dataset, you can augment your dataset and make fake training examples. You can also take the dataset of images of numbers and apply random distortions on it to create new data.

There is also another regularization technique, that is early stopping. What you are going to do is run GD, and plot either the training error or the value of the cost function that should decrease monotonically. You would also, in addition to the previous plot, plot the dev set error. This can be the classification error of the dev set or something like the cost function. What you will see is that the dev set error will go down for a bit, then increase from a point. What early stopping will do is you will say that you will stop the training at the point when the dev set error will start to increase. So this happens because, in the initial iterations, the values of W will be very small, but as you train the NN, W will get bigger and bigger. So, with early stopping, you will stop the training at a point where W may not be so large and be medium sized. By keeping your values of W relatively small, your squared norm will also be relatively smaller.

But this process has a downside. ML can be broken down into many steps. Two of the main ones can be the optimization of cost function J, and preventing overfitting. Thinking of these two as seperate tasks makes ML relatively easier because these two steps require completely different set of tools to accomplish. This principle is also called Orthonogalization, where you think about one task at a time (more on this later). But with early stopping, you are trying to accomplish both at the same time, you are trying to minimize the cost function as much as you can, but you are also trying to prevent overfitting as much as you can. This can prevent you from working on these problems independently.