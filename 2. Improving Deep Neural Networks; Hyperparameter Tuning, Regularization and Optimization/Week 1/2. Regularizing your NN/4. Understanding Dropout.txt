To see how dropout works as a regularizer, lets look at an output unit, that takes input from 4 hidden units in the previous layer. When dropout will be applied, sometimes 2 random units will be eliminated, sometimes 1 random unit will be eliminated. So this means that the output unit cant rely on anyone feature, since any one of them can be "turned off" or eliminated. As a result, it will have to spread out it's weights, which will have the effect of skrinking the squared norm weights (the regularizing term). It turns out that dropout can be formally showned to be an adaptive form of L2 regularization, but L2 regularization applied to different ways can be a little different and even more adaptive to the scale of different inputs.

When you are implementing dropout in practice, first thing you would have to do is decide what will be the keep-prop value. It is also feasable to vary keep-prop by layer. For layer with many inputs and hidden units, you may keep the keep-prop very low like 0.5 or 0.6, then with layers with less inputs and hidden layers, you may keep the keep-prop relatively high, maybe even 1 for penultimate layer and output layers, because these are the layers which you are not worried about the overfitting. Sometimes you can also apply dropout on the input layer, but in practice, dont do that often.

Dropout is used more often in computer vision, as there isnt any data, so the algorithm almost always overfits, so some scientists use dropout almost as a default in CV problems. But, just remember that dropout is a regularization technique, so if your algorithm is not overfitting, you wouldnt use it. A big downside of dropout is that the cost function J will no longer be well defined on every iteration, because you are randomly cutting off random nodes. So it becomes harder to double check the performance of GD. Normally, if you have a well defined cost function, you can plot the graph to see if the cost is consistently going down. But, you would lose that tool when using dropout. So what you should do is when debugging GD, you can turn off dropout (set keep-prop to 1 for all layers) and then run the code to see if the graph is monotonically decreasing.