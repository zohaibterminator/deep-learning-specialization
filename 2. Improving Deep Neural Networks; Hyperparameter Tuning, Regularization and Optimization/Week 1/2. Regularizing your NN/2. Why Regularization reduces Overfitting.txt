Lets see why does regularization reduces overfitting.

If you add a regularization term that penalizes the weight matrices for having a big values, so if the value of lambda is large, this means that the majority of the values of W matrix will be close to zero, which also means for some hidden units, the output will be basically zero. This will result in your NN to behave basically like a logistic regression model because so many hidden units will be zeroing out that they will be close to 0 impact. This practice of zeroing out your hidden units is not that recommended, but it does help you build a simpler model that is less prone to overfitting. By finding an intermediate value of lambda that isnt too small or too large, will be ideal.

Another intuition is that, lets say we are using tanh as the activation function, if the value of z is very small, or the value of z is close to 0, we can see on the tanh graph the learning is pretty linear. We learned in the previous course that using the linear function as the activation function in the linear layer is useless because this will not allow the NN to fit a curve like decision boundary to the data, and we will be basically be training a glorified logistic regression or linear regression model.