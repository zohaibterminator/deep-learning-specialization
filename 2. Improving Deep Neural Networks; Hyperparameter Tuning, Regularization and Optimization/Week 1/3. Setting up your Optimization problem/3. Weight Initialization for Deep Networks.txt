One thing that helps in reducing exploding/vanishing gradients is careful choice of random initialization of the neural networks.

Before we apply this to a neural network, lets first take the example of a logistic regression model with 4 inputs. We know that z is equal to:

z = w_1*x_1 + w_2*x_2 + ... + w_n*x_n

In order to make sure that the value of Z does not blow up or be too small, the larger the n is (the number of features), the smaller the value of w_i needs to be. Since Z is equal to the sum of W_i*X_i, if there are a lot of values of W_i, you would want them to be smaler. One reasonable thing to do is to set the variance of W to be 1/n, where n will be the number of features. Normally, we would want W^[l] to be equal to:

W^[l] = np.random.randn(shape) * np.sqrt(1/n^[l-1])

If you are using ReLU, 2/n variance works better. There are some other initialization techniques like Xavier Initialization for tanh, which uses np.sqrt(1/n^[l-1]). And another one uses 2/(n^[l-1] + n^[l]). But ReLU is the most used activation functions, so its moe likely you will be using 2/n^[l-1], but if you want the variance to be another hyperparameter you can tune, you can have another number that multiplies with the variance, and you can tune that number as a hyperparameter.