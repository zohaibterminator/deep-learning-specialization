One of the main problems in training a very deep neural network are vanishing and exploding gradients. Meaning, when you are training your model, slopes or the derivatives can sometimes get very, very big or very, very small, even exponentially small.

Lets say you are training a very deep neural networks with 9 layers, each with 2 hidden units. Lets say that all of parameters from W^[1] to W^[l] are equal to a matrix slightly larger than identity matrix, lets say equal to 1.5, and lets assume b to be 0 for all layers. If you calculate Yhat, that will be equal to matrices of 1.5 to the power of l-1, multiplied by X. This means that yhat will explode and will have very large values. Now lets switch out 1.5 for 0.5, now yhat  will be equal to matrices of 0.5 to the power of l-1, multiplied by X, which will be extremely small.