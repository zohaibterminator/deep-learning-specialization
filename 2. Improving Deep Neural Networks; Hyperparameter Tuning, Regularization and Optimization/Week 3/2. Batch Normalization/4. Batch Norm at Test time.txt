Batch norm processes your data one mini batch at a time, but the test time you might need to process the examples one at a time. Within a single mini batch, you'd sum over that mini batch of the Z^(i) values to compute the mean. So here, you're just summing over the examples in one mini batch (in the image, m denotes the number of examples in a mini-batch)

So, firstly, you're just summing over the examples in one mini batch. Then, you compute the variance and then you compute Z norm by scaling by the mean and standard deviation with Epsilon added for numerical stability. And then Z̃ is calculated by taking Z norm and rescaling by gamma and beta. So, notice that mean and variance, which you need for this scaling calculation, are computed on the entire mini batch. But the test time you might not have a mini batch of 64, 128 or 256 examples to process at the same time. So, you need some different way of coming up with the valuesof  mean and variance. And if you have just one example, taking the mean and variance of that one example, doesn't make sense. So, in order to apply your neural network at test time is to come up with some separate estimate of mean and variance. And in typical implementations of batch norm, what you do is estimate this using a exponentially weighted average where the average is across the mini batches.

So, when training on X^{1} for a layer l, you get some μ^{1}[l]. And then when you train on the second mini batch for that layer and that mini batch, you end up with some second value of μ^{2}[l]. And then for the third mini batch in this hidden layer, you end up with some third value of μ^{3}[l]. So, just as we saw how to use a exponentially weighted average  of the current temperature, you would do that to keep track of what's the latest average value of this mean vector you've seen. That exponentially weighted average becomes your estimate for what the mean of the Zs is for that hidden layer and similarly, you use an exponentially weighted average to keep track of these values of σ^2 that you see on the first mini batch in that layer, σ^2 that you see on second mini batch and so on. At test time, you will just use whatever the latest value you have of μ and σ^2, and compute the Z_norm^(i).