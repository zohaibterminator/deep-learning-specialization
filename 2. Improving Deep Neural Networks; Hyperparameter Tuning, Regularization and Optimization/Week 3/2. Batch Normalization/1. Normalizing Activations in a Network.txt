Batch normalization makes your hyperparameter search problem much easier, makes your neural network much more robust. The choice of hyperparameters is a much bigger range of hyperparameters that work well, and will also enable you to much more easily train even very deep networks.

Previously, we looked at normalizing the input features and how it speeds up the learning of, for example, a logistic regression model. Now, if we have a deeper network with 3 hidden layers and 1 output layer, we can normalize, lets say, the mean and variance of a^[2], which is the input of layer 3, then we can make training of W^[3] and b^[3] more efficient. This is called batch normalization or batch norm for short.

To implement batch norm, we will actually normalize the values of z^[l], not a^[l]. There is a debate as to which method is better, normalizing after activation, or before it. We will go with normalization before activation. The steps of the normalization is the same for the most part, but after calculating the norm of Z, we will have another step. Why we are doing this is that we dont want the hidden units to have mean 0 and variance 1, we want each hidden unit to have a different distribution. This is because if we look at the sigmoid function for example, all of our values will be clustered around the middle part, which would not let us take advantage of the non-linearity of the function. What we will do is calculate Z_~^(i), whose formula is:

Z_~^(i) = γ*Z_norm^(i) + β

Here, both γ and β are learnable parameters, and you would update these parameters in the same way you would update the weights parameters. These parameters, γ and β, basically allows you to set the mean of Z_~^(i) to whatever you want it to be. Basically, if we assume that the γ is equal to square root of variance + epsilon, and β is equal to the mean of Z, then with the above equation, we are basically undoing the operations of the norm, but due to the changing values of both γ and β, we can make mean and variance of Z_~^(i) whatever we want.