We know that normalizing the input features to mean zero and variance one can speed up learning. So, one intuition behind why batch norm works is, this is doing a similar thing, but further values in your hidden units and not just for the input layer. Now, this is just a partial picture for what batch norm is doing.

One of intuition for why batch norm works is that it makes the weights of the deeper layer of the neural network more robust to changes to weights in earlier layers of the neural network. Lets say you are training a shallow network like logistic regression or maybe a deep network, on a cat detection task, But let's say that you've trained your data sets on all images of black cats. If you now try to apply this network to data with colored cats where the positive examples are not just black cats, then your classifier might not do very well. This is because the distribution of the inputs have changed, therefore, a different decision boundary will be required then to the one your model learned. So, this idea of your data distribution changing goes by the name of "Covariate Shift". The idea is that, if you've learned some X to Y mapping, if the distribution of X changes, then you might need to retrain your learning algorithm, and this is true even if the function, the ground truth function, mapping from X to Y, remains unchanged. The training of your algorithm becomes even more difficult if the ground truth function shifts as well.

So, how does this problem of covariate shift apply to a neural network? Lets say you are training a neural network, if we look at the perspective of the 3rd layer, it's activations A^[3] depends on the values of it's input from the 2nd layer, which is A^[2]. The network is also adapting parameters W^[2], B^[2] and W^[1], B^[1], and so as these parameters change, these values, A^[2], will also change. So from the perspective of the third hidden layer, these hidden unit values are changing all the time, and so it's suffering from the problem of covariate shift.

Batch Norm reduces the amount that the distribution of these hidden unit's values shifts around. And if it were to plot the distribution of these hidden unit values, maybe this is technically renormalizer Z, so this is actually Z_1^[2] and Z_2^[2], and I also plot two values instead of four values, so we can visualize in 2D. What batch norm is saying is that, the values for Z_1^[2] Z and Z_2^[2] can change, and indeed they will change when the neural network updates the parameters in the earlier layers. But what batch norm ensures is that no matter how it changes, the mean and variance of Z_1^[2] and Z_2^[2] will remain the same. So even if the exact values of Z_1^[2] and Z_2^[2] change, their mean and variance will at least stay same mean zero and variance one. Or, not necessarily mean zero and variance one, but whatever value is governed by beta γ_2 and β_2.

Batch Norm also has a slightly regularization effect. So one non-intuitive thing about batch norm is that each mini-batch X^{t}, has the values Z^{t}, has the values Z^[l], scaled by the mean and variance computed on just that one mini-batch. Now, because the mean and variance computed on just that mini-batch as opposed to computed on the entire data set, that mean and variance has a little bit of noise in it, because it's computed just on your mini-batch. So because the mean and variance is a little bit noisy because it's estimated with just a relatively small sample of data, the scaling process, going from Z^[l] to Z_2^[l], that process is a little bit noisy as well. So similar to dropout, it adds some noise to each hidden layer's activations. The way dropout has noises, it takes a hidden unit and it multiplies it by zero with some probability, and multiplies it by one with some probability. So your dropout has multiplicative noise because it's multiplied by zero or one, whereas batch norm has multiplicative noise because of scaling by the standard deviation, as well as additive noise because it's subtracting the mean. Well, here the estimates of the mean and the standard deviation are noisy. And so, similar to dropout, batch norm therefore has a slight regularization effect. Because by adding noise to the hidden units, it's forcing the downstream hidden units not to rely too much on any one hidden unit. Because the noise added is quite small, this is not a huge regularization effect, and you might choose to use batch norm together with dropout if you want the more powerful regularization effect of dropout. And maybe one other slightly non-intuitive effect is that, if you use a bigger mini-batch size, right, so if you use use a mini-batch size of, say, 512 instead of 64, by using a larger mini-batch size, you're reducing this noise and therefore also reducing this regularization effect. So that's one strange property of dropout which is that by using a bigger mini-batch size, you reduce the regularization effect. But, dont use it as a regularizer.