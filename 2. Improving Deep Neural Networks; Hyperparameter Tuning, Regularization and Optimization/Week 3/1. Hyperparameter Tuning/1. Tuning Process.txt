One of the painful things about training deepness is the sheer number of hyperparameters you have to deal with, ranging from the learning rate alpha to the momentum term beta, if using momentum, or the hyperparameters for the Adam Optimization Algorithm which are beta one, beta two, and epsilon. You will also need to find the right number of layers and hidden units, maybe you also need to implement learning rate decay. Finally, you might also need to find mini-batch size.

Of all the hyperparameters, learning rate is the most important hyperparameter. After learning rate, the momentum term (even though 0.9 is a good default value), mini-batch size and the number of hidden units are the hyperparameters you should focus on. After you tuned the above hyperparameters, tuning the number of layers can also sometimes make a huge difference, and also the learning rate decay. It is not recommended to tune beta_1, beta_2 and epsilon parameters, but you can tune them after you have tuned all other hyperparameters if you want.

Now how do you select what range of values should you experiment with when tuning your hyperparameters. In the early days, you would make a grid, and then fill the grid with different pairs of hyperparameters. Then you would systematically go over the grid and see which pairing works best, and then choose that. But this works only if you have a small number of hyperparameters. In DL, we tend to choose random values for hyperparameters, and then see what works well. In this way, you can try out a variety of different values, and you get a hgher chance of finding a good value for a hyperparameter.

When you are sampling hyperparameters, another common practice is to use a coarse to fine sampling scheme. Lets say in a 2D grid, you find that the points in a certain region gives you better results than points in other regions, so you what you will do is shift your focus from the whole grid, to that small area, and then sample more densely, still at random, for good hyperparameter values in that region.