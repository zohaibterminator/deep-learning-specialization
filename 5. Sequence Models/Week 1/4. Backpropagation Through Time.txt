In backprop, as you might already have guessed, you end up carrying backpropagation calculations in basically the opposite direction of the forward prop. In order to compute backpropagation, you need a loss function. So let's define an element-wise loss function, which is, suppose for a certain word in the sequence, if it is a person's name, y^<t> is 1, and your RNN will output a probability ŷ of that particular word being a person's name. So, we will define a standard logistic loss, also called the binary cross entropy loss:

L^<t> = -y^<t> * log(ŷ^<t>) - (1 - y^<t>) * log(1 - ŷ^<t>)

If we want to define an overall loss function, or the cost function, we would just sum the loss at every timestep. Now, in this back propagation procedure, the most significant message or the most significant recursive calculation is the updation through the connection through which the activation is passed to the next timestep, which goes from right to left, and that's why it gives this algorithm the name called backpropagation through time. And the motivation for this name is that for forward prop, you are scanning from left to right, increasing indices of the time, t, whereas, the backpropagation, you're going from right to left, you're kind of going backwards in time.