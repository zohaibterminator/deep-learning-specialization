Language modeling is one of the most basic and important task in natural language processing. It is also one that RNNs do very well.

What is a language model? Let's say you're building a speech recognition system and you hear the sentence, the apple and pear salad was delicious. What did you hear the person say? Did they say the apple and pair salad? Or did they say the apple and pear salad? You probably think the second sentence is much more likely. In fact, that's what a good speech recognition system would output, even though these two sentences sound exactly the same. The way a speech recognition system picks the second sentence is by using a language model which tells it what is the probability of either of these two sentences. For example, a language model might say that the chance of the first sentences is 3.2 x 10^-13, and the chance of the second sentence is say 5.7 x 10^-10, and so with these probabilities, the second sentence is much more likely by over a factor of 10^3 compared to the first sentence, and that's why a speech recognition system will pick the second choice.

What a language model does is, given any sentence, its job is to tell you what is the probability of that particular sentence, and by probability of sentence, I mean, if you were to pick up a random newspaper, open a random email, or pick a random webpage, or listen to the next thing someone says, the friend of you says, what is the chance that the next sentence you read somewhere out there in the world will be a particular sentence like the apple and pear salad? This is a fundamental component for both speech recognition systems as you've just seen, as well as for machine translation systems, where translation systems want to output only sentences that are likely:

P(y^<1>, y^<2>, y^<3>, ..., y^<Ty>) = ?

To build such a model using a RNN, you will first need a training set comprising a large corpus of English text or text from whatever language you want to build a language model of. The word corpus is an NLP terminology that just means a large body or a very large set of English sentences. Let's say you get a sentence in your training set "cats average 15 hours of sleep a day". The first thing you would do is tokenize the sentence, and that means you would form a vocabulary as we saw earlier, and then map each of these words to one-hot vectors or to indices in your vocabulary. So another common thing to do is to add an extra token called <EOS> that stands for end of sentence, that can help you figure out when a sentence ends. The EOS token can be appended to the end of every sentence in your training set if you want your model to explicitly capture when sentences end. Now, one other detail would be, what if some of the words in your training set are not in your vocabulary? If your vocabulary uses 10,000 words, maybe the 10,000 most common words in English, then the term Mau as a decision, Mau's breed of cat, that might not be in one of your top 10,000 tokens. In that case, you could take the word Mau and replace it with a unique token called UNK, which stands for unknown words, and we just model the chance of the unknown word instead of the specific word, Mau.

Having carried out the tokenization step, which basically means taking the input sentence and map them to the individual tokens or the individual words in your vocabulary, next, let's build an RNN to model the chance of these different sequences. One of the things we'll see next is that you end up setting the inputs x^<t> to be equal to y^<t-1>. At time zero, you're going to end up computing some activation a^<1> as a function of some input x^<1>, and x^<1> would just be set to zero vector. The previous a^<0> by convention, also set that to vector zeros. But what a_1 does is it will make a Softmax prediction to try to figure out what is the probability of the first word y, so that's going to be ŷ^<1>. What this step does is really it is a Softmax, so it's trying to predict what is the probability of all word in a dictionary being the first word of the sentence, including the chance for <UNK> and <EOS> tokens. So, it will be 10,002 dimensional output which will have all the probabilities of the words in the dictionary, including <UNK> and <EOS> tokens, and we will perform argmax to find the word with the biggest probability, and that will be the model's prediction for the first word of the sentence.

Then the RNN steps forward to the next step and has some activation a^<2> in the next step. At this step, it's job is to try to figure out what is the second word. But now we will also give it the correct first word. We'll tell it that in reality, the first word was actually cats, so x^<2> will be y^<1>, the first word of our sentence. Now, ŷ^<2> will again be a vector of 10,002 dimensions having all the probabilities of the words in the vocabulary, but these will be the probabilities of the words, given what words have come previously, which, in our case, is cats. Then you go on to the next step of the RNN where you now compute a_3. But to predict what is the third word of the sentence, which is 15, we can now give it the first two words. We will then pass 'average' into the RNN, which is the second word of the sentence, and the RNN in turn will again output a vector of probabilities of the words in the vocab given the first 2 words, cats and average.

So, each step in the RNN will look at some set of preceding words such as, given the first three words, what is the distribution over the next word? This RNN learns to predict one word at a time going from left to right. Next, to train this through a network, we're going to define the cost function. We will use the standard cross entropy loss function, which will calculate the loss given the true word y^<t> and a predicted word ŷ^<t> at a certain time step t, and then the overall loss is just the sum over all time steps of the losses associated with the individual predictions. If you train this RNN on a large training set, what it will be able to do is, given any initial set of words such as "cats average 15" or "cats average 15 hours of", it can predict what is the chance of the next word. You will get the probabilities of P(y^<1>), P^(y^<2>|y^<1>) and P^(y^<3>|y^<2>, y^<1>), then you will just multiply all the probabilities to get P(y^<1>, y^<2>, y^<3>).