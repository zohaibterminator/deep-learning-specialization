Previously, we learned about the GRU, the Gated Recurring Unit and how that can allow you to learn very long range connections in a sequence. The other type of unit that allows you to do this very well is the LSTM or the long short term memory units. And this is even more powerful than the GRU.

Firstly, one thing in LSTMs that is different than GRUs is that here we seperate the a^<t> and c^<t>, meaning the memory cell is a completely seperate entity compared to the activations. So, the major change is in the equation of the candidate memory cell:

c'^<t> = tanh(Wc * [a^<t-1>, x^<t>] + bc)

Here, we get rid of the relevant or reset gate and instead use the activations of the previous time step, and we also use a^<t> instead of c^<t>. There is also a similar change in update gate equation which that we use the activation a^<t> in the calculations instead of c^<t>:

Γu = σ(Wu * [a^<t-1>, x^<t>] + bu)

But, in LSTMs we add 2 new gates, the forget gate and the output gate. The forget gate is used in the calculations to create the new memory cell instead of (1 - Γu), meaning the forget gate actually controls how much of the previous memory we retained and how much do we "forget", hence called the forget gate:

Γf = σ(Wf * [a^<t-1>, x^<t>] + bf)

The output gate is actually used to create the new value of the activations from the newly created memory cell after passing it through the tanh function, which is also passed on alongside the memory cell to the next time step:

Γo = σ(Wo * [a^<t-1>, x^<t>] + bo)

c^<t> = Γu * c'^<t> + Γf * c^<t-1>

a^<t> = Γo * tanh(c^<t>)

So, the LSTMs use 3 gates compared to 2 used in GRUs, and as a result is a bit more complicated. The activation a^<t-1> and x^<t> is used to compute the values for all the gates. One thing about the LSTMs and GRUs is that if line up multiple units together and connect them, you notice is that so long as you set the forget and the update gates, appropriately, it is relatively easy for the LSTM to have some value c^<0> and have that be passed all the way to the right to have, maybe c^<3> equals c^<0>. And this is why the LSTM as well as the GRU is very good at memorizing certain values. Even for a long time for certain real values stored in the memory cells even for many, many times steps.

There are also a few variations on this that people use. Perhaps the most common one, is that instead of just having the gate values be dependent only on a^<t-1>, x^<t>. Sometimes people also sneak in there the value c^<t-1> as well. This is called a peephole connection. If you see peephole connection, what that means is that the gate values may depend not just on a t-1 but and on x t but also on the previous memory cell value. And the peephole connection can go into all three of these gates computations.

When should you use a GRU and when should you use an LSTM. There is a widespread consensus in this. Researchers have tried both of these models on many different problems and on different problems the different algorithms will win out. So there isn't a universally superior algorithm. the advantage of the GRU is that it's a simpler model. And so it's actually easier to build a much bigger network only has two gates, so computation runs a bit faster so it scales the building, somewhat bigger models. But the LSTM is more powerful and more flexible since there's three gates instead of two. If you want to pick one to use, I think LSTM has been the historically more proven choice. So if you had to pick one, I think most people today will still use the LSTM as the default first thing to try. Although I think the last few years GRUs have been gaining a lot of momentum and I feel like more and more teams are also using GRUs because they're a bit simpler but often were, just as well and it might be easier to scale them to even bigger problems.