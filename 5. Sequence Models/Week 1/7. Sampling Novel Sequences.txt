After you train a sequence model, one of the ways you can informally get a sense of what is learned is to have a sample novel sequences.

So remember that a sequence model, models the chance of any particular sequence of words P(y^<1>, y^<2>, ..., y^<t>), and so what we like to do is sample from this distribution to generate novel sequences of words. To sample, you do something slightly different, so what you want to do is first sample what is the first word you want your model to generate. And so for that you input the usual x^<1> equals 0, a^<0> equals 0. So what you do is you then randomly sample according to this softmax distribution. So, the model gives you a vector of probabilities, and then you take this vector and use, for example, the numpy command np.random.choice to sample according to distribution defined by this vector probabilities, and that lets you sample the first words. Next you then go on to the second time step, and now remember that the second time step is expecting y^<1>, the first word of the target sentence, as input. Since we are sampling and not trying to predict the probability of the words given some sentence, you instead pass the first sampled word to the next time step as input.

And so how do you know when the sequence ends? Well, one thing you could do is if the end of sentence token is part of your vocabulary, you could keep sampling until you generate an EOS token. And that tells you you've hit the end of a sentence and you can stop. Or alternatively, if you do not include this in your vocabulary then you can also just decide to sample 20 words or 100 words or something, and then keep going until you've reached that number of time steps. And this particular procedure will sometimes generate an unknown word token. If you want to make sure that your algorithm never generates this token, one thing you could do is just reject any sample that came out as unknown word token and just keep resampling from the rest of the vocabulary until you get a word that's not an unknown word. So this is how you would generate a randomly chosen sentence from your RNN language model.

Now, so far we've been building a words level RNN, by which I mean the vocabulary are words from English. Depending on your application, one thing you can do is also build a character level RNN. So in this case your vocabulary will just be the alphabets. Up to z, and as well as maybe space, punctuation if you wish, the digits 0 to 9. And if you want to distinguish the uppercase and lowercase, you can include the uppercase alphabets as well, and one thing you can do as you just look at your training set and look at the characters that appears there and use that to define the vocabulary. And if you build a character level language model rather than a word level language model, then your sequence y^<1>, y^<2>, y^<3>, would be the individual characters in your training data, rather than the individual words in your training data. Using a character level language model has some pros and cons. One is that you don't ever have to worry about unknown word tokens. In particular, a character level language model is able to assign a sequence like mau, a non-zero probability. Whereas if mau was not in your vocabulary for the word level language model, you just have to assign it the unknown word token. But the main disadvantage of the character level language model is that you end up with much longer sequences. So many english sentences will have 10 to 20 words but may have many, many dozens of characters. And so character language models are not as good as word level language models at capturing long range dependencies between how the the earlier parts of the sentence also affect the later part of the sentence. And character level models are also just more computationally expensive to train.

So the trend we've been seeing in natural language processing is that for the most part, word level language model are still used, but as computers gets faster there are more and more applications where people are, at least in some special cases, starting to look at more character level models. But they tend to be much harder, much more computationally expensive to train, so they are not in widespread use today.