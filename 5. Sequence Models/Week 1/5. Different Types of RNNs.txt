So far, we've seen an RNN architecture where the number of inputs, Tx, is equal to the number of outputs, Ty. It turns out that for other applications, Tx and Ty may not always be the same, so now, you'll see a much richer family of RNN architectures.

We saw previously, that for many problems, the length of the input sequence and the output sequence may not be the same. For example, in sentiment classification, given a sequence of words, the output can just be an integer in the range 1-5. And in name entity recognition, in the example we're using, the input length and the output length are identical, but there are also some problems were the input length and the output length can be different. In machine translation, where a French sentence and English sentence may need two different numbers of words to say the same thing.

The example we have seen so far, in which we take a sequence of input x^<t>, and pass them through the RNN one by one, and get a output sequence in return. So, this is what you might call a many-to-many architecture because the input sequence has many inputs as a sequence and the outputs sequence is also has many outputs. If we take another problem like Sentiment Classification, we will have a sequence of words as input, which can be review for a movie, and we want the RNN to predict whether it is a positive review or a negative review, so a binary classification problem. So, when we pass the input sequence through the RNN, we may not want the RNN to output a sequence at every step, but to look at all the words, at give an output, 1 or 0, at the last time step. This is called the many-to-one architecture. Because it inputs many words and but it just outputs one number. For the sake of completeness, there is also a one-to-one architecture. This is like a normal NN that we covered earlier. Now, in addition to many-to-one, you can also have a one-to-many architecture.

So an example of a one-to-many neural network architecture will be music generation. And the input x could be maybe just an integer, telling it what genre of music you want or what is the first note of the music you want, and if you don't want to input anything, x could be a null input, a vector of zeroes.  And then, have your RNN output the first value, and then, have that, with no further inputs, output the second value and then go on to output, the third value, and so on, until you synthesize the last notes of the musical piece. One technical note, when you're actually generating sequences, often you take the synthesized output and feed it to the next layer as well.  It turns out there's one more interesting example of many-to-many which is worth describing. Which is when the input and the output length are different. For an application like machine translation, the number of words in the input sentence, say a French sentence, and the number of words in the output sentence, say the translation into English, those sentences could be different lengths. So here's an alternative NN architecture where you might have a NN, first, read the sentence. And having done that, you then, have the NN output the translation. And so, this NN architecture has two distinct parts. There's the encoder which takes as input, say a French sentence, and then, there's is a decoder, which having read the sentence, outputs the translation into a different language.