We've learned about how RNNs work and how they can be applied to problems like name entity recognition as well as to language modeling. We also saw how back propagation can be used to train an RNN. It turns out that one of the problems of the basic RNN algorithm is that it runs into vanishing gradient problems. Let's discuss that and next we'll talk about some solutions that will help to address this problem.

Let's take a language modeling example. Lets say we have 2 sentences, "The cat, which has already ate ....., is already full" and "The cats, which already ate ....., are already full". This is one example of when language can have very long-term dependencies where a word that occured much earlier can affect what needs to come much later in the sentence. But it turns out that the basic RNN we've seen so far is not very good at capturing very long-term dependencies. To explain why, remember from our earlier discussions of training very deep neural networks that we talked about the vanishing gradients problem. So, if you have a very deep network, with 100 layers, the loss from the output layer will have a hard time propagating back to affect the weights of the earlier layers.

For an RNN with a similar problem, you have forward prop going from left to right and then backprop going from right to left. It can be quite difficult because of the same vanishing gradients problem for the outputs of the errors associated with the later timesteps to affect the computations that are earlier. In practice, what this means is it might be difficult to get a neural network to realize that it needs to memorize that did it see a singular noun or a plural noun so that later on in the sequence it can generate either was or were, depending on whether it was singular or plural. Because of this problem, the basic RNN model has many local influences, meaning that the output ŷ^<3> is mainly influenced by values close to ŷ^<3>. This is a weakness of the basic RNN algorithm, one which we will address later.

Even though this discussion has focused on vanishing gradients, you remember, when we're talking about very deep neural networks that we also talked about exploding gradients. Where doing backprop, the ingredients should not just decrease exponentially they may also increase exponentially with the number of layers you go through. It turns out that vanishing gradients tends to be the biggest problem with training RNNs. Although when exploding gradients happens it can be catastrophic because the exponentially large gradients can cause your parameters to become so large that your neural network parameters get really messed up. It turns out that exploding gradients are easier to spot because the parameter has just blow up. You might often see NaNs, not a numbers, meaning results of a numerical overflow in your neural network computation. If you do see exploding gradients, one solution to that is apply gradients clipping. All that means is, look at your gradient vectors, and if it is bigger than some threshold, re-scale some of your gradient vectors so that it's not too big, so that is clipped according to some maximum value. That's a relatively robust solution that will take care of exploding gradients.