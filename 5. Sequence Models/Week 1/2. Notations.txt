Let's start by defining a notation that we'll use to build up these sequence models. As a motivating example, let's say you want to build a sequence model to input a sentence "Harry Potter and Hermione Granger invented a new spell". And let say you want a sequence model to automatically tell you where are the peoples names in this sentence. So, this is a problem called Named-entity recognition and this is used by search engines for example, to index all of say the last 24 hours news of all the people mentioned in the news articles so that they can index them appropriately. The NER systems can be used to find people's names, companies names, times, locations, countries names, currency names, and so on in different types of text.

Now, given this input x let's say that you want a model to output y that has one outputs per input word and the target output y tells you for each of the input words whether it is part of a person's name. So, the output of the sentence we defined previously will be: [1 1 0 1 1 0 0 0 0]. And technically this maybe isn't the best output representation, there are some more sophisticated output representations that tells you not just whether a word is a part of a person's name, but tells you where the start and ends of people's names in the sentence, as maybe you want to know where Harry Potter starts and ends. But we will stick to our output representation for this example.

Now, the input is the sequence of nine words. So, eventually we're going to have nine sets of features to represent these nine words, and to represent the index of the words, we will represent them as x^<i> with the angle brackets. So, if we want to represent the first word of the input text sequence in the form of an index, we can do so using x^<1>. We will use the index t to index into positions in the middle of the sequence. We will also be using the same indexing notation for output sequence y. We will use Tx to denote the length of the input sequence x, and use Ty to denote the length of the output sequence y. As you already know, we are using X^(i) to denote the ith example in the training set, so if we write X^(i)<t>, we are denoting the t-th element in the input sequence of the i-th training example. Similarly, we will denote Tx^(i) to denote the length of the sequence of the i-th training example. Again, we will also use the same notation for the target sequence in the training examples.

Let's next talk about how we would represent individual words in a sentence. So, to represent a word in the sentence the first thing you do is come up with a Vocabulary. Sometimes also called a Dictionary and that means making a list of the words that you will use in your representations. So the first word in the vocabulary is 'a', that will be the first word in the dictionary. The second word is 'Aaron' and then a little bit further down is the word 'and', and then eventually you get to the words 'Harry' then eventually the word 'Potter', and then all the way down to maybe the last word in dictionary is 'Zulu'. And so, 'a' will be word one, 'Aaron' is word two, and in my dictionary the word 'and' appears in positional index 367. 'Harry' appears in position 4075, 'Potter' in position 6830, and 'Zulu' is the last word to the dictionary is maybe word 10,000. So in this example, we will be going to use a dictionary with size 10,000 words. This is quite small by modern NLP applications. For commercial applications, dictionary sizes of 30 to 50,000 are more common and 100,000 is not uncommon. And then some of the large Internet companies will use dictionary sizes that are maybe a million words or even bigger than that. But we will be going to use 10,000 for illustration since it's a nice round number.

So, if you have chosen a dictionary of 10,000 words and one way to build this dictionary will be be to look through your training sets and find the top 10,000 occurring words, also look through some of the online dictionaries that tells you what are the most common 10,000 words in the English Language. Then, you can use one hot representations to represent each of these words. For example, x^<1> which represents the word Harry would be a vector with all zeros except for a 1 in position 4075 because that was the position of Harry in the dictionary. And then x^<2> will be again similarly a vector of all zeros except for a 1 in position 6830 and then zeros everywhere else. Each of these vectors will be of 10,000 dimensions, if your vocabulary has 10,000 words. So, you will have 9 vectors to represent our example input sequence, and the goal is given this representation for X to learn a mapping using a sequence model to then target output y, and we will do this as a supervised learning problem, given the tabular data with both x and y. Now, what if you encounter a word that is not in your vocabulary? Well the answer is, you create a new token or a new fake word called Unknown Word <UNK> to represent words not in your vocabulary.