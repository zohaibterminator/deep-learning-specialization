The different versions of RNNs we've seen so far will already work quite well by themselves. But for learning very complex functions sometimes is useful to stack multiple layers of RNNs together to build even deeper versions of these models.

Lets say we have a sentence of 4 words, and we want the RNN to make a prediction based on that. Lets say we use 3 RNNs and stack them on top of each other. For the prediction ŷ^<1> at time step 1, the input x^<1> will need to be passed through all the 3 layers of the RNNs before the final prediction is made. All the layers will have seperate parameters, but a particular layer's parameters will be the same across the time steps, same as an RNN.

For RNNs, having three layers is already quite a lot. Because of the temporal dimension, these networks can already get quite big even if you have just a small handful of layers. And you don't usually see these stacked up to be like 100 layers. One thing you do see sometimes is that you have recurrent layers that are stacked on top of each other. But then you might take the output, and then just have a bunch of deep layers that then finally predicts ŷ^<1>. And you can have the same deep network that predicts ŷ^<2>.

And quite often, these blocks don't just have to be standard RNN, the simple RNN model. They can also be GRU blocks LSTM blocks. And finally, you can also build deep versions of the bidirectional RNN. Because deep RNNs are quite computationally expensive to train, there's often a large temporal extent already, though you just don't see as many deep recurrent layers.