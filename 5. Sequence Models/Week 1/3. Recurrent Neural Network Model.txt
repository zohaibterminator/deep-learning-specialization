If we want to learn the mappings from a given input sequence x to an output sequence y, one thing we could do is to try to use a standard NN for this task. So, in our example, we had nine input words.  So, you could imagine trying to take these nine input words, maybe the nine one-hot vectors and feeding them into a standard neural network, maybe a few hidden layers, and then eventually had this output the nine values zero or one that tell you whether each word is part of a person's name.

But this turns out not to work well and there are really two main problems of this approach. The first is that the inputs and outputs can be different lengths and different examples. So, it's not as if every single example has the same input length Tx or the same output length Ty and maybe if every sentence has a maximum length, then you could pad or zero-pad every inputs up to that maximum length but this still doesn't seem like a good representation. And then a second and maybe more serious problem is that a naive NN architecture doesn't share features learned across different positions of texts. In particular if the NN has learned that maybe the word Harry appearing in position one gives a sign that that's part of a person's name, then wouldn't it be nice if it automatically figures out that Harry appearing in some other position x^<t> also means that that might be a person's name. And this is maybe similar to what you saw in convolutional neural networks where you want things learned for one part of the image to generalize quickly to other parts of the image, and we like a similar effects for sequence data as well. And similar to what you saw with ConvNets using a better representation will also let you reduce the number of parameters in your model, because if even if we pad the sequences to a max length Tx, if we have a vocabulary of 10,000 words, we would have a matrix of Tx x 10,000 which means the number of parameters required will be huge, which is similar to the problem we encountered when we used normal NN for images. A recurrent NN does not have either of these disadvantages. 

If we feed a word to RNN from a sequence, lets say, x^<1>, it will try to predict the output sequence y^<1>. Then, when we feed it the next word in the sequence, i.e. x^<2>, it won't only predict y^<2> based off of just x^<1>, it also gets as input some information from what it had the computer in the previous step, or in time step one. So in particular, the activation value from time step one is passed on to time step two. Then at the next time step, recurrent neural network inputs the third word x^<3> and it tries to output some prediction, ŷ^<3> and so on up until the last time step where it inputs x^<Tx> and then it outputs ŷ^<Ty>. In this example, Tx is equal to Ty and the architecture will change a bit if Tx and Ty are not identical. So at each time step, the recurrent neural network that passes on as activation to the next time step for it to use. And to kick off the whole thing, we'll also have some either made-up activation at time zero, this is usually the vector of zeros. Some researchers will initialized a^[0] randomly. You have other ways to initialize a^[0] but really having a vector of zeros as the fake times zero activation is the most common choice.

The recurrent neural network scans through the data from left to right. The parameters it uses for each time step are shared. The parameters governing the connection from x^<1> to the hidden layer, will be some set of parameters we're going to write as Wax and is the same parameters Wax that it uses for every time step. The activations that gets passed on to the next step, those connections will be governed by some parameters called Waa and the same parameters Waa will be used on every timestep, and similarly there's some parameters Wya that governs the output predictions.

Now, one weakness of this RNN is that it only uses the information that is earlier in the sequence to make a prediction. In particular, when predicting y^<3>, it doesn't use information about the words x^<4>, x^<5>, x^<6> and so on.  So this is a problem because if you are given a sentence, "He said Teddy Roosevelt was a great president." In order to decide whether or not the word Teddy is part of a person's name, it would be really useful to know not just information from the first two words but to know information from the later words in the sentence as well because the sentence could also have been, "He said teddy bears they're on sale." So given just the first three words is not possible to know for sure whether the word Teddy is part of a person's name. In the first example, it is. In the second example, it is not. But you can't tell the difference if you look only at the first three words. We will see how we can address this issues later using Bidirectional RNNs.

Now, we will see how the forward prop is calculated in RNNs. We know that the activation at time step zero a^<0> is a vector of zeros, so the forward prop starts from time step 1. In time step 1, to calculate the activation a^<1>, we will use this equation:

a^<1> = g(Waa * a^<0> + Wax * x^<1> + ba)

Then, to predict the output ŷ^<1>, we will use the following equation:

ŷ^<1> = g(Wya * a^<1> + by)

The activation function used to compute a^<i> is usually tanh, and although ReLU is also used, tanh is the common choice. And, depending on what your output y^<i> is, that is, if its a binary classification problem, you will use sigmoid function to predict the output label, or a softmax function if it is a k-way classification problem. So, generally, the equations can be written as:

a^<t> = g(Waa * a^<t-1> + Wax * x^<t> + ba)
ŷ^<t> = g(Wya * a^<t> + by)

Now, to simplify the notation, we will actually stack the Wax and Waa weight matrices horizontally and call it the Wa matrix. Lets say if the activation matrix is of 100 dimensions, and the input x^<t> is of 10,000 dimensions, then the size of Waa matrix would be 100x100, and the size of Wax will be 100x10,000. If we stack them up horizontally, with Waa at the left and Wax at the right, we would get the matrix Wa of size 100x10,100. To, multiply the matrix Wa with a^<t> and x^<t>, we would then stack them vertically, with a^<t> at the top and x^<t> at the bottom, to make it a 10,100 x 100 matrix. Now, if multiply the two values, we will get the result for the Waa * a^<t-1> + Wax * x^<t> equation. Using this, we can now simplify the equation for a^<t>:

a^<t> = g(Wa * [a^<t-1>, x^<t>] + ba)