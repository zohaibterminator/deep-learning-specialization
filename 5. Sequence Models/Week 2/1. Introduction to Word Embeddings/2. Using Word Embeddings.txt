We learned what it might mean to learn a featurized representations of different words. Now, we will see how we can take these representations and plug them into NLP applications.

Continuing with the named entity recognition example, if you're trying to detect people's names. Given a sentence like "Sally Johnson is an orange farmer", hopefully, you'll figure out that "Sally Johnson" is a person's name. And one way to be sure that "Sally Johnson" has to be a person, rather than say the name of the corporation is that you know "orange farmer" is a person. So previously, we had talked about one hot representations to represent these words, x^<1>, x^<2>, and so on. But if you can now use the featurized representations, the embedding vectors that we talked about, then after having trained a model that uses word embeddings as the inputs, if you now see a new input, "Robert Lin is an apple farmer". Knowing that orange and apple are very similar will make it easier for your learning algorithm to generalize to figure out that "Robert Lin" is also a human, is also a person's name.

One of the most interesting cases will be, what if in your test set you see not "Robert Lin is an apple farmer", but you see much less common words? What if you see "Robert Lin is a durian cultivator"? A durian is a rare type of fruit, popular in Singapore and a few other countries. But if you have a small label training set for the named entity recognition task, you might not even have seen the word durian or seen the word cultivator in your training set. But if you have learned a word embedding that tells you that durian is a fruit, so it's like an orange, and a cultivator, someone that cultivates is like a farmer, then you might still be generalize from having seen an orange farmer in your training set to knowing that a durian cultivator is also probably a person.

So one of the reasons that word embeddings will be able to do this is the algorithms for learning word embeddings can examine very large text corpuses, maybe found off the Internet. So you can examine very large data sets, maybe a billion words, maybe even up to 100 billion words would be quite reasonable. So very large training sets of just unlabeled text. And by examining tons of unlabeled text, which you can download more or less for free, you can figure out that orange and durian are similar. And farmer and cultivator are similar, and therefore, learn embeddings, that groups them together. Now having discovered that orange and durian are both fruits by reading massive amounts of Internet text, what you can do is then take this word embedding and apply it to your named entity recognition task, for which you might have a much smaller training set, maybe just 100,000 words in your training set, or even much smaller. And so this allows you to carry out transfer learning, where you take information you've learned from huge amounts of unlabeled text to figure out that orange, apple, and durian are fruits. And then transfer that knowledge to a task, such as named entity recognition, for which you may have a relatively small labeled training set. Alternatively, you can also directly download learned word embeddings like Word2Vec or GloVe instead of training your own model to generate these embeddings. One other thing you can also do, although it is optional, is continue to fine-tune the embeddings with the new data. You should do this only if the labelled data for your task is big enough, otherwise you don't need to fine-tune the embeddings furthur. One nice thing also about word embeddings is you can now use relatively lower dimensional feature vectors. So rather than using a 10,000 dimensional one-hot vector, you can now instead use maybe a 300 dimensional dense vector.

Finally, word embeddings has a interesting relationship to the face encoding ideas that we learned about in the previous course on convolutional neural networks. So you will remember that for face recognition, we train a Siamese network architecture that would learn, for example, a 128 dimensional representation for different faces. And then you can compare these encodings in order to figure out if these two pictures are of the same face. The words encoding and embedding mean fairly similar things. So in the face recognition literature, people also use the term encoding to refer to these vectors, f(x^(i)) and f(x^(j)). One difference between the face recognition literature and what we do in word embeddings is that, for face recognition, you wanted to train a neural network that can take as input any face picture, even a picture you've never seen before, and have a neural network compute an encoding for that new picture. Whereas what we'll do for learning word embeddings is that we'll have a fixed vocabulary of, say, 10,000 words. And we'll learn a vector e^1 through, say, e^10,000 that just learns a fixed encoding or learns a fixed embedding for each of the words in our vocabulary.