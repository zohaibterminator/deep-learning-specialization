Logistic regression is a shallow neural network, because it technically only has 1 layer, the output layer. But a neural network with 5 layers is a deep neural network. Through the years the AI and ML community has realized that there are functions that deep neural networks can learn that are hard to learn for a shallow neural network. But, it is difficult to decide whether a shallow or a deep neural network is needed to solve a problem, so you can kind of view the number of hidden layers as a hyperparameter, and you decide through experiments which number of layers works the best.

Lets see some notations to describe the deep neural networks. "l" is going to be used to describe the number of layers in a deep neural network. n^[l] is going to describe the number of unirs or neurons in the layer l. The input layer is also called layer 0, and the output layer is also called layer L. n^[0] will then be equal to the number of features or n_x. Similarly, we denote the activations of a layer l by a^[l], so therefore, a^[l] will be equal to g^[l](z^[l]), where g^[l]() is the activation function of layer l. Again, we will use similar notations to refer to the parameters of a layer.