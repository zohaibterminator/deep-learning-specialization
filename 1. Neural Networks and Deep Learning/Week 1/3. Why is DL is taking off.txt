Already discussed in ML Spec Course 2.

In addition to more having access to more data and computation resources, algorithms played a huge role in making the performance of NNs much better. Let's say you train a NN with a sigmoid function, if you compare it to the performance of a NN with a ReLU activation function, the second NN will perform much better. Because the curve at the end of the sigmoid function makes the learning little bit slower, meanwhile the way ReLU "turns off" for values smaller than 0 really helps in training a NN.