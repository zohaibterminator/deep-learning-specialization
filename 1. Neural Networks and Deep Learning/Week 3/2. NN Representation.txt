We will look at how NN are represented, specifically NN with one hidden layer.

A NN has different parts. There's the input features stacked up vertically, which are referred to as the input layer, which contains the input of the NN. Then we have the hidden layer, more discussion on that later, and then we have the output layer. In our case, we only have 1 node in the output layer, which outputs the prediction for us.

As to why the "middle" layers are called hidden layers, the term hidden layer refers to the fact that in the training set, the true values for these nodes in the middle are not observed. That is, you don't see what they should be in the training set. You see what the inputs are. You see what the output should be. But the things in the hidden layer are not seen in the training set. So that kind of explains the name hidden layer; just because you don't see it in the training set.

We were using X to denote the input features, we will now call it a^[0]. The a stands for activations, which refers to the values that are pass on to one layer their subsequent layer. So the input layer passes on the input X to the hidden layer. The hidden layer will also generate some acivations, which we will call a^[1]. This activation will be vector whose number of dimensions will base on the number of nodes or neurons in the layer. For example if the hidden layer has 4 neurons, a^[1] will have 4 dimension containing values [a_1^[1], a_2^[1], a_3^[1], a_4^[1]]. This will be passed on to the output layer, which will produce a^[2], which will be just a number as the layer only has one node. This NN with an input layer, one hidden layer and one output layer is called a 2 layer NN, because we don't the input layer as an official layer. As discussed earlier, each layer will have parameters associated with them. For our hidden layer, the parameters will be W^[1] and b^[1], with dimensions 4x3 and 4x1 respectively, where the first coordinate four comes from the fact that we have four nodes of our hidden units and a layer, and three comes from the fact that we have three input features. Similarly, the output layer also has parameters associated with it and they are W^[2] and b^[2], with their dimensions being 1x4 and 1x1 respectively. 