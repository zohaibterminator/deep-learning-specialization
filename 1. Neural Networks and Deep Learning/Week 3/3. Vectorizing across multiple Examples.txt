The normal, inefficient version of the prediction of the neural network we discussed earlier is this:

for i=1 to m:
    z^[1](i) = w^[1] * x^(i) + b^[1]
    a^[1](i) = sigmoid(z^[1](i))
    z^[2](i) = w^[2] * a^[1](i) + b^[2]
    a^[2](i) = sigmoid(z^[2](i))

The vectorized implementation would be:

Z^[1] = W^[1] * X^ + b^[1]
A^[1] = sigmoid(Z^[1])
Z^[2](i) = W^[2] * A^[1] + b^[2]
A^[2](i) = sigmoid(Z^[2])

Inorder to vectorize this, we will first stack all the m training examples into a matrix like this:

    [ .      .     .         .   ]
X = [x^(1) x^(2) x^(3) .... x^(m)]
    [ .      .     .         .   ]

And also do the same for the Z^[1] and A^[1]:

    [   .         .         .            .   ]
Z = [z^[1](1) z^[1](2) z^[1](3) .... z^[1](m)]
    [   .         .         .            .   ]

In this way, horizontally, A will represent the activation for different training examples, and vertically, it will represent the activation for the hidden units. For example, the value at the top left will represent the activation outputed from the first hidden unit for the first training example. Similarly, this applied to Z as well. For X matrix, horizontally, it will represent the number of training examples, meanwhile vertically it will represent the features.