Lets say we just remove the activation function, so that a^[i] = z^[i]. Sometimes this is called the linear activation function. It turns out that if you do this, the model is just computing yhat as a linear function of your input features x. So a hidden layer with no activation function or linear activation function is more or less useless, because the composition of two linear function is a linear function. One place where you might use linear activation function is in the output layer if you are performing regression like predicting housing prices, etc. Then the hidden layers should use ReLU, Leaky ReLU or tanh activation function.