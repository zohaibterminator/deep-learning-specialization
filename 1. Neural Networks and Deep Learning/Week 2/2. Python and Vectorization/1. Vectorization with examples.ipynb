{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Vectorization is the art of removing explicit for loops in your code. When applying deep learning in practice, you will train models on very large datasets, because thats when DL algorithms tend to shinit. So, it's important that your code is optimized, because otherwise, you will have to wait a long time to get the results.\n",
    "\n",
    "In logistic regression, you have to calculate `z`, which is the result of `w.T * x + b`. If you implement it in a non-vectorized way, you would probably initialize a for loop to run `n_x` times, and multiply each individual value of `w` and `x`, and add b before adding it to `z`. This is a very inefficient implementation. If you will implement a vectorized example, the whole operation can just be performed not only using a single line of code, but also way more efficiently using this:\n",
    "\n",
    "`z = np.dot(w, x) + b`\n",
    "\n",
    "Lets look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7837753295898438 milliseconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(f\"{1000*(toc-tic)} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Vectorized example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795.2377796173096 milliseconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "c = 0\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(f\"{1000*(toc-tic)} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot DL implementations are done using GPU, but we just used CPU using Jupyter notebook. Well, both CPU and GPU have parallelized instructions or SIMD instructions. So, if you use this  `numpy` function like `dot` and others, it enables Python NumPy to take advantage of the parallelism to do your computations much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Vectorization Examples\n",
    "\n",
    "If you want to calculate u as the product of matrix A and vector v, the general rule of matrix calculation like this would be Î£(A(i,j) * v(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 milliseconds\n"
     ]
    }
   ],
   "source": [
    "A = np.arange(start=0, stop=25, step=1).reshape(5, 5)\n",
    "v = np.ones((5, 1), dtype='int64')\n",
    "\n",
    "tic1 = time.time()\n",
    "u = np.dot(A, v)\n",
    "# u = np.matmul(A, v) another function to do the same thing\n",
    "toc1 = time.time()\n",
    "\n",
    "print(f\"{1000*(toc1-tic1)} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-vectorized example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.014823913574219 milliseconds\n"
     ]
    }
   ],
   "source": [
    "u = np.zeros((5, 1), dtype='int64')\n",
    "A = np.arange(start=0, stop=25, step=1).reshape(5, 5)\n",
    "v = np.ones((5, 1), dtype='int64')\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        u[i] += A[i, j]*v[j]\n",
    "toc = time.time()\n",
    "\n",
    "print(f\"{1000*(toc-tic)} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, whenever you are thinking of making a for loop, just see if there is some `numpy` function that can do it for you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
