We will cover basics of neural network programming like vectorization and the intuition of organizing computation of a neural network in the form of a forward pass, or foward propagration, and a backward pass, or backpropagation through logistic regression.

Lets take an example of a binary classification problem. You have an input for the image, and you want to recognize that image as, for example, a cat, in which case you output 1, or not a cat, in which case you output 0. This image is stored in the form of 3 seperate matrices, corresponding to the red, green and blue pixel intensity values of the image. If the image is 64x64 in size, then the computer will store this as 3 seperate 64x64 matrices for each color. In order to turn these values into a feature vector, we will unroll these values into a single column. First we will have all the red values then green values then blue values. If the size of the image is 64x64, then the total dimension of the feature vector is 64 x 64 x 3, because that is the total number of numbers we have in these matrices, which is 12288. We will represent n_x or n as the dimension of the input features, which in this case will be 12288.

In binary classification, we want to build a classifier that takes in the feature vector x, and predicts whether the corresponding y label is either 1 or 0, or in this case, whether it is a cat image or not. Usually the data is stacked in a column instead of a matrix, like we did in the above example, when training a neural network. We will do the same when training a NN.