We talked about how you want your learning algorithm to do well on the training set but sometimes you don't actually want to do too well and knowing what human level performance is, can tell you exactly how well but not too well you want your algorithm to do on the training set.

Again, we will be using the cat classifier example. Lets say your cat classifier has achieved 8% error on the training set and 5% error on the dev set. Lets also say, that humans have near-perfect accuracy, so the human-level error is 1%. Knowing the human-level error, you want your algorithm to decrease it's training error, or in other words, you want to decrease your algorithm's bias.

Lets take another example of a different dataset. Lets say that your training and dev set error are the same, but, now lets assume that the human-level error on the dataset is 7.5% instead of 1%. Maybe the images in the dataset are so blurry even humans couldn't classify them correctly. Now, even though the training error and the dev error are the same, you will think that maybe your algorithm is doing fine on the training set. In this example, you may want to focus on reducing the performance gap between training and dev error, or in other words, focus on reducing the variance of the algorithm.

In the previous examples when we discussed bias and variance, we assumed that the bayes error is nearly zero. But, actually, think of human-level error as a proxy/estimate for Bayes optimal error. And for CV tasks, this is a reasonable proxy, because humans are really good at CV tasks, so the human-level error may not be far from the Bayes optimal error.

The term that is used to describe the difference between the human-level error (approx. Bayes optimal error) and the training error is called Avoidable bias, and the differnce between the training error and the dev error is called variance. The term Avoidable bias acknowledges the fact that there is some bias or some minimum level of error that you cant overcome.