The term human-level performance is sometimes used casually in research articles. But now you will learn how we can define it a bit more precisely. And in particular, use the definition of the phrase, human-level performance, that is most useful for helping you drive progress in your machine learning project.

Human-level error, is that it gives us a way of estimating Bayes error. So bearing that in mind, let's look at a medical image classification example. Let's say that you want to look at a radiology image like this, and make a diagnosis classification decision. And suppose that:

* A typical, untrained, human achieves a 3% error on the task.
* A typical radiologist doctor achieves a 1% error on the task.
* An experienced doctor achieves a 0.7% error on the task.
* A team of experienced doctor achieves a 0.5% error on the task.

So how would you define human-level error? 3%, 1%, 0.7%, or 0.5%? The correct answer will be the 0.5% error. By definition, the lowest possible error that can be achievable by a system, in this case a team of experienced doctors, is 0.5%, so the Bayes optimal error will be   0.5% or lower. Maybe an even larger team of doctors can achieve better results but we know that the Bayes optimal error can't be greater than 0.5%. So, we would define the human-level error as 0.5%. Now, for the purpose of publishing a research paper or for the purpose of deploying a system, maybe there's a different definition of human-level error that you can use which is so long as you surpass the performance of a typical doctor. That seems like maybe a very useful result if accomplished, and maybe surpassing a single radiologist, a single doctor's performance might mean the system is good enough to deploy in some context. So maybe the takeaway from this is to be clear about what your purpose is in defining the term human-level error.

Let's say, for a medical imaging diagnosis example, that your training error is 5% and your dev error is 6%. Depending on whether you defined it as a typical doctor's performance or experienced doctor or team of doctors, you would have either 1% or 0.7% or 0.5% for this. So in this first example, whichever of these choices you make, the measure of avoidable bias will be something like 4%. It will be somewhere between I guess, 4%, if you take that to 4.5%, if you use 0.5%. Whether this is 4% or 4.5%, this is clearly bigger than the variance problem, so in this case, you should focus on reducing the bias of your network.

But where it really matters will be if your training error is 0.7%. So you're doing really well now, and your dev error is 0.8%. In this case, it really matters that you use your estimate for Bayes error as 0.5%. Because in this case, your measure of how much avoidable bias you have is 0.2% which is twice as big as your measure for your variance, which is just 0.1%. And so this suggests that maybe both the bias and variance are both problems but maybe the avoidable bias is a bit bigger of a problem. And in this example, 0.5% as we discussed on the previous slide was the best measure of Bayes error, because a team of human doctors could achieve that performance. If you use 0.7 as your proxy for Bayes error, you would have estimated avoidable bias as pretty much 0%, and you might have missed that you actually should try to do better on your training set. So I hope this gives a sense also of why making progress in a machine learning problem gets harder as you achieve or as you approach human-level performance. In this example, once you've approached 0.7% error, unless you're very careful about estimating Bayes error, you might not know how far away you are from Bayes error. And therefore how much you should be trying to reduce aviodable bias. In fact, if all you knew was that a single typical doctor achieves 1% error, and it might be very difficult to know if you should be trying to fit your training set even better.