It's not always easy to combine all the things you care about into a single row number evaluation metric. In those cases its sometimes useful to set up satisficing as well as optimizing metrics.

Lets say you have 3 cat classifiers, and you are measuring their accuracy and running time. Now, its not as simple as using a formula that combines the two metrics into a single number metric, so what you will do is define which metric is your optimizing metric, i.e. the metric you want to maximise, and which is your satisficing metric, i.e. the metric needs to be good enough or to be at a certain acceptable level.

In this case, accuracy will be our maximising metric, meaning we want our accuracy to get higher and higher, and running time will be our satisficing metric. We can set a limit of 100 milliseconds, for example, so that a classifier has to have a runing time of 100 milliseconds or less. So this will be a pretty reasonable way to trade off or to put together accuracy as well as running time. And it may be the case that so long as the running time is less that 100 milliseconds, your users won't care that much whether it's 100 milliseconds or 50 milliseconds or even faster. And by defining optimizing as well as satisficing metrics, this gives you a clear way to pick the, quote, "best classifier".

More generally, if you have N metrics that you care about it's sometimes reasonable to pick one of them to be optimizing. So you want to do as well as is possible on that one. And then N minus 1 to be satisficing, meaning that so long as they reach some threshold such as running times faster than 100 milliseconds, but so long as they reach some threshold, you don't care how much better it is in that threshold.