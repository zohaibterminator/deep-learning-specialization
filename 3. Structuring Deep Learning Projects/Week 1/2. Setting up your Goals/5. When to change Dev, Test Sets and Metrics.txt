To have a dev set and evaluation metric is like placing a target somewhere for your team to aim at. But sometimes partway through a project you might realize you put your target in the wrong place. In that case you should move your target.

Let's say you build a cat classifier to try to find lots of pictures of cats to show to your cat loving users and the metric that you decided to use is classification error. So algorithms A and B have, respectively, 3% error and 5% error, so it seems like Algorithm A is doing better. But let's say you try out these algorithms, you look at these algorithms and Algorithm A, for some reason, is letting through a lot of the pornographic images. So if you shift Algorithm A the users would see more cat images because you'll see 3 percent error and identify cats, but it also shows the users some pornographic images which is totally unacceptable both for your company, as well as for your users. In contrast, Algorithm B has 5 percent error so this classifies fewer images but it doesn't have pornographic images. So from your company's point of view, as well as from a user acceptance point of view, Algorithm B is actually a much better algorithm because it's not letting through any pornographic images.

So, what has happened in this example is that Algorithm A is doing better on your evaluation metric. It's getting 3 percent error but it is actually a worse algorithm. So, in this case, the evaluation metric plus the dev set prefers Algorithm A because they're saying, look, Algorithm A has lower error which is the metric you're using but you and your users prefer Algorithm B because it's not letting through pornographic images. So when this happens, when your evaluation metric is no longer correctly rank ordering preferences between algorithms, in this case is mispredicting that Algorithm A is a better algorithm, then that's a sign that you should change your evaluation metric or perhaps your development set or test set.

In this case the misclassification error metric that you're using can be written as follows:

Error = 1/m_dev * Sum(y_pred^(i) != y^(i))

The problem with this eval metric is that it treats pornographic and non-pornographic images equally. But you really want your classifier to not label pornographic images incorrectly as a cat picture, and show it to a user. You can alter the eval metric by adding a weight:

Error = 1/(sum(w^(i))) * Sum(w^(i) * y_pred^(i) != y^(i))

Where w^(i) is 1 if X^(i) is not porn, and a large number (like a 10 or a 100) if X^(i) is porn, so you are giving a much larger weight to images that identify as pornographic. To implement such weighting scheme you would actually have to go through the dev and test sets and label pornographic images as pornographic, but don't worry about the implementation details of this solution.

But the high level of take away is, if you find that your evaluation metric is not giving the correct rank order preference for what is actually a better algorithm, then there's a time to think about defining a new evaluation metric (and this is just one possible way that you could define an evaluation metric). The goal of the evaluation metric is to accurately tell you, given two classifiers, which one is better for your application.

One thing you might notice is that so far we've only talked about how to define a metric to evaluate classifiers. That is, we've defined an evaluation metric that helps us better rank order classifiers when they are performing at varying levels in terms of streaming out porn. And this is actually an example of an orthogonalization, where we have now broken down the problem into 2 distinct steps:

* Figure out a metric that captures what you want to do.
* Worry seperately on how to do well on that metric.

Let's look at just one more example. Let's say that your two cat classifiers A and B have, respectively, 3 percent error and 5 percent error as evaluated on your dev set. Or maybe even on your test set which are images downloaded off the internet, so high quality well framed images. But maybe when you deploy your algorithm product, you find that algorithm B actually looks like it's performing better, even though it's doing better on your dev set. And you find that you've been training off very nice high quality images downloaded off the Internet but when you deploy those on the mobile app, users are uploading all sorts of pictures, they're much less framed, you haven't only covered the cat, the cats have funny facial expressions, maybe images are much blurrier, and when you test out your algorithms you find that Algorithm B is actually doing better. So this would be another example of your metric and dev test sets falling down. The problem is that you're evaluating on the dev and test set as very nice, high resolution, well-framed images but what your users really care about is you have them doing well on images they are uploading, which are maybe less professional shots and blurrier and less well framed. So the guideline is, if doing well on your metric and your current dev sets or dev and test sets' distribution, if that does not correspond to doing well on the application you actually care about, then change your metric and/or your dev test set.

So it is recommended that even if you can't define the perfect evaluation metric and dev set, just set something up quickly and use that to drive the speed of your team iterating. And if later down the line you find out that it wasn't a good one, you have better idea, change it at that time, it's perfectly okay. But what is recommended against for the most teams is to run for too long without any evaluation metric and dev set up because that can slow down the efficiency of what your team can iterate and improve your algorithm.