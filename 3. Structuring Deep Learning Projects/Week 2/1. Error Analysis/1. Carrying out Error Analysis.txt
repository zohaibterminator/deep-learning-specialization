If you're trying to get a learning algorithm to do a task that humans can do. And if your learning algorithm is not yet at the performance of a human. Then manually examining mistakes that your algorithm is making, can give you insights into what to do next. This process is called error analysis.

Lets say you are evaluating your cat classifier (yes, again), and you are getting 90% accuracy, or conversely, 10% error, on the dev set, and it's much worse than what you are hoping to do. Your teammate looks at the examples that the classifier is getting wrong, and they notice that your cat classifier is miscategorizing some dogs as cats. So, they come to you with a proposal for making the algorithm do better, specifically on dogs. You can imagine building a focus effort, maybe to collect more dog pictures, or maybe to design features specific to dogs, or something, in order to make your cat classifier do better on dogs, so it stops misrecognizing these dogs as cats. So the question is, should you go ahead and start a project focused on the dog problem? There could be several months of works you could do in order to make your algorithm make fewer mistakes on dog pictures. So is that worth your effort?

Rather than spend a few months doing this, only to risk finding out that it wasn't that helpful, you can do some error analysis and find out if this will be worth your time. Get around a 100 mislabeled dev set examples, then examine them manually and count them up to see how many of them are actually pictures of dogs. Lets say you find that mislabeled dog pictures make up for 5% of the 100 mislabeled dev set examples, so about 5 pictures. Even if you solve the problem of mislabeled dog pictures, your classifier is getting about 5 more examples correct of the 100 mislabeled examples you found. So your error can go down by about 0.5% percent, down to 9.5%. So you might reasonably decide that this isn't the best use of your time, or rather it gives you a "Performance Ceiling", as it called in ML, which just means, what's in the best case? How well could working on the dog problem help you? Lets say, instead of 5%, you found that 50% of the mislabeled examples are dog pictures, now the impact is bigger. Now if you work on solving the dog problem, you could decrease the error by about half, down to potentially 5% error from 10%.

Sometimes you can also evaluate multiple ideas in parallel doing error analysis. For example, let's say you have several ideas in improving your cat detector. Maybe you can improve performance on dogs? Or maybe you notice that sometimes, what are called great cats, such as lions, panthers, cheetahs, and so on. That they are being recognized as small cats, or house cats. So you could maybe find a way to work on that. Or maybe you find that some of your images are blurry, and it would be nice if you could design something that just works better on blurry images. So, to perform error analysis for these 3 ideas, you can make a table in a spreadsheet (a simple txt file could also work), and you can make 3 seperate columns for the 3 ideas, one for Dogs, one for Great Cats, and one for Blurry images, and you can also make an additional column for comments. If the first picture is blurry, you make a check mark on the corresponding row in the blurry column, and maybe write a comment to remember what the picture looked like. If the second picture is a dog, you will make a check mark on the corresponding row in the dog column, and write some comments (like pitbull, if the dog in the image is a pitbull to help you remember it). If a picture is of a lion and it is also blurry, you can make a check mark in both the column of the great cats and the blurry images for that image. At the end, you will count which percentage of each of the error categories were attributed to dog, to the great cats and to the blurry images. Lets say that 8% of the images were dogs, 43% were great cats and 61% of the images were blurry. As you're part way through this process, sometimes you notice other categories of mistakes. So, for example, you might find that Instagram style filter, those fancy image filters, are also messing up your classifier. In that case, it's actually okay, part way through the process, to add another column like that. For the multi-colored filters, the Instagram filters, and the Snapchat filters. And then go through and count up those as well, and figure out what percentage comes from that new error category.

The conclusion of this process gives you an estimate of how worthwhile it might be to work on each of these different categories of errors. For example, clearly in this example, a lot of the mistakes were made on blurry images, and quite a lot on were made on great cat images. And so the outcome of this analysis is not that you must work on blurry images. This doesn't give you a rigid mathematical formula that tells you what to do, but it gives you a sense of the best options to pursue. It also tells you, for example, that no matter how much better you do on dog images, or on Instagram images. You at most improve performance by maybe 8%, or 12%, in these examples. Now there's a ceiling in terms of how much you could improve performance, is much higher. So depending on how many ideas you have for improving performance on great cats, on blurry images. Maybe you could pick one of the two, or if you have enough personnel on your team, maybe you can have two different teams. Have one work on improving errors on great cats, and a different team work on improving errors on blurry images. This quick counting procedure, which you can often do in, at most, small numbers of hours. Can really help you make much better prioritization decisions, and understand how promising different approaches are to work on.