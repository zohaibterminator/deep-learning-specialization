If your training set comes from a different distribution, than your dev and test set, and if error analysis shows you that you have a data mismatch problem, what can you do? There aren't completely systematic solutions to this, but let's look at some things you could try.

When you have a data mismatch problem, you will need to carry out error analysis and see the differences between the training set and the dev/test sets (or technically only the training and the dev set). Lets say you are working on the speech activated rearview mirror, you will need to listen to the speech in both training and dev sets to understand the difference between them. For example, you find that the examples in the dev set has a lot of car noise. You may also find other categories of errors such that your system is also miscategorizing the navigational queries in your dev set, because there are a lot more navigational queries in the dev set compared to the training set.

What you can do is the make the training data more similar to the dev/test sets. Or, alternatively, you can collect more data similar to the dev and test sets. So if you find that the car noise in the background is a major source of error, then you can simulate car noise. Or if you are having a hard time recognizing street numbers, then you can go and deliberately try to get more data of people speaking out numbers.

This isn't a systematic process and there's no guarantee that you get the insights you need to make progress. But I have found that this manual insight, together we're trying to make the data more similar on the dimensions that matter that this often helps on a lot of the problems. So if you want to make your training data similar to the dev/test data, one of the techniques you can use is to perform artificial data synthesis.

To build a speech recognition system, maybe you don't have a lot of audio that was actually recorded inside the car with the background noise of a car, background noise of a highway, and so on. But, it turns out, there's a way to synthesize it. So, let's say that you've recorded a large amount of clean audio without this car background noise. Here's an example of a clip you might have in your training set. >>The quick brown fox jumps over the lazy dog. But, given that recording of "the quick brown fox jumps over the lazy dog," you can then also get a recording of car noise. And if you take these two audio clips and add them together, you can then synthesize what saying "the quick brown fox jumps over the lazy dog" would sound like, if you were saying that in a noisy car. So, this is a relatively simple audio synthesis example. In practice, you might synthesize other audio effects like reverberation which is the sound of your voice bouncing off the walls of the car and so on. But through artificial data synthesis, you might be able to quickly create more data that sounds like it was recorded inside the car without needing to go out there and collect tons of data, maybe thousands or tens of thousands of hours of data in a car that's actually driving along. So, if your error analysis shows you that you should try to make your data sound more like it was recorded inside the car, then this could be a reasonable process for synthesizing that type of data to give you a learning algorithm.

Now, there is one note of caution I want to sound on artificial data synthesis which is that, let's say, you have 10,000 hours of data that was recorded against a quiet background. And, let's say, that you have just one hour of car noise. So, one thing you could try is take this one hour of car noise and repeat it 10,000 times in order to add to this 10,000 hours of data recorded against a quiet background. If you do that, the audio will sound perfectly fine to the human ear, but there is a risk that your learning algorithm will over fit to the one hour of car noise.

Here's another example of artificial data synthesis. Let's say you're building a self driving car and so you want to really detect vehicles. In particular, one idea that a lot of people have independently raised is, once you find a video game with good computer graphics of cars and just grab images from them and get a huge data set of pictures of cars, it turns out that if you look at a video game, if the video game has just 20 unique cars in the video game, then the video game looks fine because you're driving around in the video game and you see these 20 other cars and it looks like a pretty realistic simulation. But the world has a lot more than 20 unique designs of cars, and if your entire synthesized training set has only 20 distinct cars, then your neural network will probably overfit to these 20 cars.