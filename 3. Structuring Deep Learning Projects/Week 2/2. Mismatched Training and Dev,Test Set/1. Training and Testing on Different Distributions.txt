Deep learning algorithms have a huge hunger for training data. They just often work best when you can find enough label training data to put into the training set. This has resulted in many teams sometimes taking whatever data you can find and just shoving it into the training set just to get it more training data. Even if some of this data, or even maybe a lot of this data, doesn't come from the same distribution as your dev and test data. So in a deep learning era, more and more teams are now training on data that comes from a different distribution than your dev and test sets. And there's some subtleties and some best practices for dealing with when you're training and test distributions differ from each other.

Let's say that you're building a mobile app where users will upload pictures taken from their cell phones, and you want to recognize whether the pictures that your users upload from the mobile app is a cat or not. So you can now get two sources of data, one which is the distribution of data you really care about, this data from a mobile app, which tends to be less professionally shot, less well framed, maybe even blurrier because it's shot by amateur users. The other source of data you can get is you can crawl the web and just download a lot of, for the sake of this example, let's say you can download a lot of very professionally framed, high resolution, professionally taken images of cats. And let's say you don't have a lot of users yet for your mobile app. So maybe you've gotten 10,000 pictures uploaded from the mobile app. But by crawling the web you can download huge numbers of cat pictures, and maybe you have 200,000 pictures of cats downloaded off the Internet.

One thing you care about is that you want your system to do well on pictures uploaded through the users mobile phones. Because, in the end, your users will be uploading images taken from their phones. But you now have a delimma, because you only have 10,000 examples of the images taken by users, and 200,000 images taken from the internet. You can't just use the images given by the users, because that will make for a small dataset. If you only use the images from the internet, than the chances of your algorithms doing well on classifying images taken by users are slim, because the two images aren't from the same distribution.

One option you have is to combine the dataset. So you will have 210,000 images in total, counting both the images from the internet and the images taken by the users, which you will randomly shuffle into train, dev and test set. Lets say your training set will have 205,000 examples, while each of your dev and test sets will have 2500 examples. This option has some advantages and also some disadvantages. The advantage is that all your train, dev and test sets will come from the same distribution, which makes it easier to manage. The huge disadvantage of this will be that a lot of your dev set will consist of images from the web page distribution rather than from the mobile app distribution. So all of these 2,500 examples on expectation, I think 2,381 of them will come from web pages. This is on expectation, the exact number will vary around depending on how the random shuttle operation went. But on average, only 119 will come from mobile app uploads. So remember that setting up your dev set is telling your team where to aim the target. And the way you're aiming your target, you're saying spend most of the time optimizing for the web page distribution of images, which is really not what you want.

Instead, what you will do is, keep the same splits, but dedicate the entire dataset of images from the web pages towards the train set, and divide the mobile app images, of which one half will go towards the train split, and the other half will be equally divided into dev and test sets. So, the splits remain the same, train set still has 205,000 examples, and the dev and test sets have the same 2500 examples each. The advantage of this way of splitting up your data into train, dev, and test, is that you're now aiming the target where you want it to be. You're telling your team, my dev set has data uploaded from the mobile app and that's the distribution of images you really care about, so let's try to build a machine learning system that does really well on the mobile app distribution of images. The disadvantage, of course, is that now your training distribution is different from your dev and test set distributions. But it turns out that this split of your data into train, dev and test will get you better performance over the long term.

Let's say you're building a brand new product, a speech activated rearview mirror for a car. So you can basically talk to the rearview mirror and basically say, dear rearview mirror, please help me find navigational directions to the nearest gas station and it'll deal with it. So how can you get data to train up a speech recognition system for this product? Well, maybe you've worked on speech recognition for a long time so you have a lot of data from other speech recognition applications, just not from a speech activated rearview mirror. Here's how you could split up your training and your dev and test sets. So for your training, you can take all the speech data you have that you've accumulated from working on other speech problems, such as:

* Data you have purchased over the years from various speech recognition data vendors (you can now buy data from these types of vendors in the form of X, Y, where X is the audio and Y is the transcript)

* Smart voice-activated speaker

* Voice-activated keyboards.

and you have 500,000 examples from all of these sources.

For the dev/test set, you have 20,000 examples that actually came from a speech activated rearview mirror. You will make the splits similarly to the way you did in the previous example. You will take the 500,000 examples from purchased data, smart voice activated speaker data, etc, and dedicate it to the training set. Then, you will take the speech activated rearview mirror data, and divide it, of which one half will go towards the train split, and the other half will be equally divided into dev and test sets. So the splits will be 510k examples for the training set, and 5k examples each for the dev and test sets.