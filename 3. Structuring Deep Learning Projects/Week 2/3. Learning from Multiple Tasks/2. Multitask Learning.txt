In transfer learning, you have a sequential process where you learn from task A and then transfer that to task B. In multi-task learning, you start off simultaneously, trying to have one neural network do several things at the same time. And then each of these task helps hopefully all of the other task.

Let's say you're building an autonomous vehicle, building a self driving car. Then your self driving car would need to detect several different things such as pedestrians, detect other cars, detect stop signs and also detect traffic lights. If we take an example of an image, there is a stop sign in that image and there is a car in that image but there aren't any pedestrians or traffic lights. So if this image is an input for an example, x^(i), then instead of having one label y^(i), you would actually a four labels. In this example, there are no pedestrians, there is a car, there is a stop sign and there are no traffic lights. And if you try and detect other things, there may be y^(i) has even more dimensions. But for now let's stick with these four. So y^(i) is a 4 by 1 vector. And if you look at the training test labels as a whole, then similar to before, we'll stack the training data's labels horizontally as follows, y^(1) up to y^(m). Except that now y^(i) is a 4 by 1 vector so each of these is a tall column vector. And so this matrix Y is now a 4 x m matrix, whereas previously, when y was single real number, this would have been a 1 x m matrix. So what you can do is now train a neural network to predict these values of y.

you can have a neural network input x and output now a four dimensional value for y. Therefore, the output layer will have 4 nodes. So the first node when we try to predict is there a pedestrian in this picture. The second output will predict is there a car here, predict is there a stop sign and the last node will predict maybe is there a traffic light. ŷ will be a four dimensional vector.

So to train this neural network, you now need to define the loss for the neural network. And so given a predicted output ŷ^(i) which is 4 x 1 dimensional. The loss averaged over your entire training set would be 1 / m sum from i = 1 through m, sum from j = 1 through 4 of the losses of the individual predictions. So it's just summing over at the four components of pedestrian, car, stop sign, traffic lights. And this script L is the usual logistic loss. And the main difference compared to the earlier binding classification examples is that you're now summing over j equals 1 through 4. And the main difference between this and softmax regression, is that unlike softmax regression, which assigned a single label to single example, whereas this one image can have multiple labels. So you're not saying that each image is either a picture of a pedestrian, or a picture of car, a picture of a stop sign, picture of a traffic light. You're asking for each picture, does it have a pedestrian, or a car a stop sign or traffic light, and multiple objects could appear in the same image. If you train a neural network to minimize this cost function, you are carrying out multi-task learning. Because what you're doing is building a single neural network that is looking at each image and basically solving four problems.

And one other detail, so far I've described this algorithm as if every image had every single label. It turns out that multi-task learning also works even if some of the images we'll label only some of the objects. So the first training example, let's say someone, your labeler had told you there's a pedestrian, there's no car, but they didn't bother to label whether or not there's a stop sign or whether or not there's a traffic light. And maybe for the second example, there is a pedestrian, there is a car, but again the labeler, when they looked at that image, they just didn't label it, whether it had a stop sign or whether it had a traffic light, and so on. And maybe some examples are fully labeled, and maybe some examples, they were just labeling for the presence and absence of cars so there's some question marks, and so on. So with a data set like this, you can still train your learning algorithm to do four tasks at the same time, even when some images have only a subset of the labels and others are sort of question marks. And the way you train your algorithm, even when some of these labels are question marks or really unlabeled is that in this sum over j from 1 to 4, you would sum only over values of j with a 0 or 1 label. So whenever there's a question mark, you just omit that term from summation but just sum over only the values where there is a label. And so that allows you to use datasets like this as well.