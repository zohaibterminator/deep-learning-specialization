One of the most powerful ideas in deep learning is that sometimes you can take knowledge the neural network has learned from one task and apply that knowledge to a separate task. So for example, maybe you could have the neural network learn to recognize objects like cats and then use that knowledge or use part of that knowledge to help you do a better job reading x-ray scans. This is called transfer learning.

Let's say you've trained your neural network on image recognition. So you first take a neural network and train it on X Y pairs, where X is an image and Y is some object. An image is a cat or a dog or a bird or something else. If you want to take this neural network and adapt, or we say transfer, what is learned to a different task, such as radiology diagnosis, meaning really reading X-ray scans, what you can do is take this last output layer of the neural network and just delete that and delete also the weights feeding into that last output layer and create a new set of randomly initialized weights just for the last layer and have that now output radiology diagnosis. So to be concrete, during the first phase of training when you're training on an image recognition task, you train all of the usual parameters for the neural network, all the weights, all the layers and you have something that now learns to make image recognition predictions. Having trained that neural network, what you now do to implement transfer learning is swap in a new data set X Y, where now these are radiology images. And Y are the diagnoses you want to predict and what you do is initialize the last layers' weights, and then retrain the neural network on the new data.

Now there are some ways you can retrain the neural net. If you have a small dataset of radiology images, you might just want to train only the last layer's weights, and keep the rest of the weights fixed. But if you have a big dataset of radiology images, you can train the rest of the weights as well. If you have a large dataset and you decide to train the entire neural net, then the phase of training of the network for image recognition will be called pre-training, because the the weights of the neural network were pre-trained on an image recognition dataset, then the retraining of the weights will be called fine-tuning. So, in this way, you have taken the knowledge gained from image recognition and transfered it to learn radiology diagnosis.

In some cases, you might just not replace the output layer of a pre-trained neural net, you might remove the output layer, and add some more layers along with another output layer. Sometimes you might also add more neurons in the output layer then they were in the original output layer.

Transfer learning makes sense when you have a lot of data for the problem you're transferring from and usually relatively less data for the problem you're transferring to. So for example, let's say you have a million examples for image recognition task. So that's a lot of data to learn a lot of low level features or to learn a lot of useful features in the earlier layers in neural network. But for the radiology task, maybe you have only a hundred examples. So you have very low data for the radiology diagnosis problem, maybe only 100 x-ray scans. So a lot of knowledge you learn from image recognition can be transferred and can really help you get going with radiology recognition even if you don't have all the data for radiology.